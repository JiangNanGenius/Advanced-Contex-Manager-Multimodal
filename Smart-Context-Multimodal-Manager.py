"""
title: ğŸš€ é«˜çº§ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆå¤šæ¨¡æ€+ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ï¼‰ - v2.6.3
author: JiangNanGenius
version: 2.6.3
license: MIT
required_open_webui_version: 0.5.17
Github: https://github.com/JiangNanGenius
description: é«˜çº§ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆä¸Šä¸‹æ–‡æœ€å¤§åŒ– + å¤šæ¨¡æ€è½¬å†™ï¼‰+ è‡ªåŠ¨è®°å¿†ï¼ˆåå°è¿è¡Œï¼Œä¸åœ¨å‰å°æ˜¾ç¤ºçŠ¶æ€ï¼‰
"""

import json
import hashlib
import asyncio
import re
import base64
import math
import time
import copy
import html
import threading
import logging
import traceback
from datetime import datetime
from typing import (
    Optional,
    List,
    Dict,
    Callable,
    Any,
    Tuple,
    Union,
    Literal,
    cast,
    Type,
    TypeVar,
)
from pydantic import BaseModel, Field, AliasChoices, ValidationError, create_model
from enum import Enum
from collections import defaultdict

# Open WebUIç›¸å…³å¯¼å…¥
from fastapi import HTTPException, Request
from open_webui.main import app as webui_app
from open_webui.models.users import UserModel, Users
from open_webui.retrieval.vector.main import SearchResult
from open_webui.routers.memories import (
    AddMemoryForm,
    MemoryUpdateModel,
    QueryMemoryForm,
    add_memory,
    delete_memory_by_id,
    query_memory,
    update_memory_by_id,
)

# å¯¼å…¥ä¾èµ–åº“
try:
    import tiktoken

    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    tiktoken = None

try:
    import httpx

    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from openai import AsyncOpenAI

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

# ========== Auto Memoryç›¸å…³ç±»å®šä¹‰ ==========


class Memory(BaseModel):
    """å•ä¸ªè®°å¿†æ¡ç›®"""

    mem_id: str = Field(..., description="è®°å¿†ID")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    update_at: datetime = Field(..., description="æ›´æ–°æ—¶é—´")
    content: str = Field(..., description="è®°å¿†å†…å®¹")
    similarity_score: Optional[float] = Field(None, description="ç›¸ä¼¼åº¦åˆ†æ•°")


class MemoryAddAction(BaseModel):
    action: Literal["add"] = Field(..., description="æ·»åŠ æ“ä½œ")
    content: str = Field(..., description="è®°å¿†å†…å®¹")


class MemoryUpdateAction(BaseModel):
    action: Literal["update"] = Field(..., description="æ›´æ–°æ“ä½œ")
    id: str = Field(..., description="è®°å¿†ID")
    # å…¼å®¹å­—æ®µåï¼šæ—¢æ¥å— contentï¼Œä¹Ÿæ¥å—å†å²ç‰ˆæœ¬çš„ new_content
    content: str = Field(
        ...,
        description="æ–°å†…å®¹",
        validation_alias=AliasChoices("content", "new_content"),
    )


class MemoryDeleteAction(BaseModel):
    action: Literal["delete"] = Field(..., description="åˆ é™¤æ“ä½œ")
    id: str = Field(..., description="è®°å¿†ID")


class MemoryActionRequestStub(BaseModel):
    """è®°å¿†æ“ä½œè¯·æ±‚"""

    actions: list[Union[MemoryAddAction, MemoryUpdateAction, MemoryDeleteAction]] = (
        Field(
            default_factory=list,
            description="æ“ä½œåˆ—è¡¨",
            max_length=20,
        )
    )
    # ä»…ç”¨äºè°ƒè¯•/è§£é‡Šï¼šå³ä½¿ actions ä¸ºç©ºï¼Œä¹Ÿè¯·ç»™å‡ºç®€çŸ­åŸå› 
    reason: str = Field(
        default="", description="Why actions is empty / rationale (debug)"
    )


# Memoryç³»ç»Ÿæç¤ºè¯
UNIFIED_SYSTEM_PROMPT = """\
You manage a per-user Memory collection (short factual notes) to personalize future chats.

Inputs you will receive:
1) Recent conversation messages (displayed with negative indices; -1 is the most recent overall message).
   Usually -2 is the user's latest message.
2) A list of existing related memories (may be empty).

Goal:
Decide what actions to take on the memory collection. Focus primarily on the user's latest message, but you MAY use the
recent surrounding context to interpret intent (e.g., ongoing goals, preferences, or commitments). Prefer 0-2 actions.

Language policy (VERY IMPORTANT):
- Write memory "content" in the same language as the user's original wording. Do NOT translate.
- If the user's latest message is in Chinese, write the memory in Chinese; if in English, write in English.
- If the user mixes languages, keep the dominant language and preserve proper nouns / product names / code / quotes verbatim.
- Write "reason" in the same language as the user's latest message (debug only).

What to remember (good candidates):
- Stable preferences: language, tone, formatting, brevity, citation style, etc.
- Ongoing projects/goals/plans that will matter in future sessions.
- Repeated workflows/habits the user is actively adopting (even if expressed as a question that signals intent).
  Example: "How do I track progress with spaced repetition?" => user is using spaced repetition and wants progress tracking.
- Constraints that shape advice: device, environment, tools, budget ranges, recurring schedules, etc.
- Explicit requests like "remember this / please remember".

What NOT to remember:
- One-off questions that do not reveal stable preferences, commitments, or background.
- Temporary states or short-lived logistics.
- Secrets/credentials/IDs. Avoid sensitive personal data (health, politics, etc.) unless the user explicitly asks you to store it.

Actions:
- ADD: Create a new memory (1 short sentence, specific, neutral).
- UPDATE: Update an existing memory (by id) when the same fact changed/refined.
- DELETE: Delete a memory (by id) when user asks to forget, or when it is clearly obsolete/duplicated.

When actions are empty, still provide a short reason for debugging.

Follow the JSON output rules provided separately.

"""

AUTO_MEMORY_OUTPUT_INSTRUCTIONS = """\
Return ONLY a valid JSON object. Do not include markdown/code fences or any extra text.

Language:
- The values of "content" and "reason" MUST preserve the user's original language. Do NOT translate.
- If the latest user message is Chinese, output Chinese; if English, output English. If mixed, keep dominant language and preserve key terms verbatim.

Schema:
{
  "actions": [
    {"action":"add","content":"<memory text>"},
    {"action":"update","id":"<id>","content":"<memory text>"},
    {"action":"delete","id":"<id>"}
  ],
  "reason": "<short explanation for debugging (even when actions is empty)>"
}

Rules:
- Always include the top-level key "actions" (can be an empty list).
- Always include "reason" (can be empty string).
- If no actions are needed, return: {"actions": [], "reason": "..."}
- For update/delete, "id" MUST be one of the provided existing IDs.
"""


STRINGIFIED_MESSAGE_TEMPLATE = "-{index}. {role}: ```{content}```"


def searchresults_to_memories(results: SearchResult) -> list[Memory]:
    """å°†æœç´¢ç»“æœè½¬æ¢ä¸ºMemoryå¯¹è±¡"""
    memories = []
    if not results.ids or not results.documents or not results.metadatas:
        raise ValueError("SearchResult must contain ids, documents, and metadatas")

    for batch_idx, (ids_batch, docs_batch, metas_batch) in enumerate(
        zip(results.ids, results.documents, results.metadatas)
    ):
        distances_batch = results.distances[batch_idx] if results.distances else None
        for doc_idx, (mem_id, content, meta) in enumerate(
            zip(ids_batch, docs_batch, metas_batch)
        ):
            if not meta:
                raise ValueError(f"Missing metadata for memory id={mem_id}")
            if "created_at" not in meta:
                raise ValueError(
                    f"Missing 'created_at' in metadata for memory id={mem_id}"
                )
            if "updated_at" not in meta:
                meta["updated_at"] = meta["created_at"]

            created_at = datetime.fromtimestamp(meta["created_at"])
            updated_at = datetime.fromtimestamp(meta["updated_at"])

            similarity_score = None
            if distances_batch is not None and doc_idx < len(distances_batch):
                similarity_score = round(distances_batch[doc_idx], 3)

            mem = Memory(
                mem_id=mem_id,
                created_at=created_at,
                update_at=updated_at,
                content=content,
                similarity_score=similarity_score,
            )
            memories.append(mem)

    return memories


def build_actions_request_model(existing_ids: list[str]):
    """åŠ¨æ€æ„å»ºè®°å¿†æ“ä½œè¯·æ±‚æ¨¡å‹"""
    if not existing_ids:
        allowed_actions = MemoryAddAction
    else:
        id_literal_type = Literal[tuple(existing_ids)]
        DynamicMemoryUpdateAction = create_model(
            "MemoryUpdateAction",
            id=(id_literal_type, ...),
            __base__=MemoryUpdateAction,
        )
        DynamicMemoryDeleteAction = create_model(
            "MemoryDeleteAction",
            id=(id_literal_type, ...),
            __base__=MemoryDeleteAction,
        )
        allowed_actions = Union[
            MemoryAddAction, DynamicMemoryUpdateAction, DynamicMemoryDeleteAction
        ]

    return create_model(
        "MemoriesActionRequest",
        actions=(
            list[allowed_actions],
            Field(
                default_factory=list,
                description="List of actions to perform on memories",
                max_length=20,
            ),
        ),
        reason=(
            str,
            Field(default="", description="Why actions is empty / rationale (debug)"),
        ),
        __base__=BaseModel,
    )


def _run_detached(
    coro, *, name: str = "detached", logger: Optional[logging.Logger] = None
):
    """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œåç¨‹ï¼ˆæ•è·å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—ï¼‰"""

    def _runner():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(coro)
        except Exception:
            if logger:
                logger.exception("Detached coroutine crashed: %s", name)
            else:
                traceback.print_exc()
        finally:
            try:
                loop.close()
            except Exception:
                pass

    threading.Thread(target=_runner, daemon=True).start()


# ========== ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç›¸å…³ç±»å®šä¹‰ ==========


class EmbeddingCache:
    """å‘é‡ç¼“å­˜å™¨ - åŸºäºcontent_keyç¼“å­˜"""

    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}

    def get(self, content_key: str) -> Optional[List[float]]:
        """è·å–ç¼“å­˜çš„å‘é‡"""
        if content_key in self.cache:
            self.access_count[content_key] = self.access_count.get(content_key, 0) + 1
            return self.cache[content_key]
        return None

    def set(self, content_key: str, embedding: List[float]):
        """è®¾ç½®ç¼“å­˜çš„å‘é‡"""
        if len(self.cache) >= self.max_size:
            least_used = min(self.access_count.items(), key=lambda x: x[1])
            del self.cache[least_used[0]]
            del self.access_count[least_used[0]]
        self.cache[content_key] = embedding
        self.access_count[content_key] = 1

    def clear(self):
        """æ¸…ç†ç¼“å­˜"""
        self.cache.clear()
        self.access_count.clear()


class MessageOrder:
    """æ¶ˆæ¯é¡ºåºç®¡ç†å™¨ - IDç¨³å®šåŒ–æ”¹è¿›"""

    def __init__(self, original_messages: List[dict]):
        self.original_messages = original_messages
        self.order_map = {}
        self.message_ids = {}
        self.content_map = {}

        for i, msg in enumerate(self.original_messages):
            content_key = self._generate_stable_content_key(msg)
            msg_id = hashlib.md5(f"{i}_{content_key}".encode()).hexdigest()[:16]
            self.order_map[msg_id] = i
            self.message_ids[i] = msg_id
            self.content_map[content_key] = i

            msg["_order_id"] = msg_id
            msg["_original_index"] = i
            msg["_content_key"] = content_key

    def _generate_stable_content_key(self, msg: dict) -> str:
        """ç”Ÿæˆç¨³å®šçš„æ¶ˆæ¯å†…å®¹æ ‡è¯†"""
        role = msg.get("role", "")
        content = msg.get("content", "")

        if isinstance(content, list):
            content_parts = []
            for item in content:
                if item.get("type") == "text":
                    content_parts.append(item.get("text", "")[:100])
                elif item.get("type") == "image_url":
                    image_data = item.get("image_url", {}).get("url", "")
                    if image_data.startswith("data:"):
                        try:
                            header, data = image_data.split("base64,", 1)
                            content_parts.append(f"[IMAGE:{header}:{data[:50]}]")
                        except:
                            content_parts.append("[IMAGE:invalid]")
                    else:
                        content_parts.append(f"[IMAGE:url:{image_data[:50]}]")
            content_str = " ".join(content_parts)
        else:
            content_str = str(content)[:200]

        return f"{role}:{content_str}"

    def generate_chunk_id(self, msg_id: str, chunk_index: int) -> str:
        """ç”Ÿæˆchunk ID"""
        return f"{msg_id}#{chunk_index}"

    def find_current_user_message_index(self, messages: List[dict]) -> int:
        """æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯çš„ç´¢å¼•"""
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("role") == "user":
                return i
        return -1

    def sort_messages_preserve_user(
        self, messages: List[dict], current_user_message: dict = None
    ) -> List[dict]:
        """æ ¹æ®åŸå§‹é¡ºåºæ’åºæ¶ˆæ¯ï¼Œä¿æŠ¤å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®"""
        if not messages:
            return messages

        other_messages = []
        current_user_in_list = None

        for msg in messages:
            if current_user_message and msg.get(
                "_order_id"
            ) == current_user_message.get("_order_id"):
                current_user_in_list = msg
            else:
                other_messages.append(msg)

        def get_order(msg):
            return msg.get("_original_index", 999999)

        other_messages.sort(key=get_order)

        if current_user_in_list:
            return other_messages + [current_user_in_list]
        else:
            return other_messages

    def get_message_preview(self, msg: dict) -> str:
        """è·å–æ¶ˆæ¯é¢„è§ˆç”¨äºè°ƒè¯•"""
        if isinstance(msg.get("content"), list):
            text_parts = []
            for item in msg.get("content", []):
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    text_parts.append("[å›¾ç‰‡]")
            content = " ".join(text_parts)
        else:
            content = str(msg.get("content", ""))

        content = content.replace("\n", " ").replace("\r", " ")
        content = re.sub(r"\s+", " ", content).strip()
        return content[:100] + "..." if len(content) > 100 else content


class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯è®°å½•å™¨"""

    def __init__(self):
        # åŸºç¡€ç»Ÿè®¡
        self.original_tokens = 0
        self.original_messages = 0
        self.final_tokens = 0
        self.final_messages = 0
        self.token_limit = 0
        self.target_tokens = 0
        self.current_user_tokens = 0

        # å¤„ç†ç»Ÿè®¡
        self.iterations = 0
        self.chunked_messages = 0
        self.summarized_messages = 0
        self.vector_retrievals = 0
        self.rerank_operations = 0
        self.multimodal_processed = 0
        self.processing_time = 0.0

        # ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç­–ç•¥ç»Ÿè®¡
        self.coverage_rate = 0.0
        self.coverage_total_messages = 0
        self.coverage_preserved_count = 0
        self.coverage_preserved_tokens = 0
        self.coverage_summary_count = 0
        self.coverage_summary_tokens = 0
        self.coverage_micro_summaries = 0
        self.coverage_block_summaries = 0
        self.coverage_upgrade_count = 0
        self.coverage_upgrade_tokens_saved = 0
        self.coverage_budget_usage = 0.0

        # åˆ†å—ä¸é¢„ç®—ç»Ÿè®¡
        self.chunked_messages_count = 0
        self.total_chunks_created = 0
        self.adaptive_blocks_created = 0
        self.block_merge_operations = 0
        self.budget_scaling_applied = 0
        self.scaling_factor = 1.0

        # æŠ¤æ ç»Ÿè®¡
        self.guard_a_warnings = 0
        self.guard_b_fallbacks = 0
        self.id_mapping_errors = 0

        # ä¸æˆªæ–­ä¿éšœç»Ÿè®¡
        self.zero_loss_guarantee = True
        self.budget_adjustments = 0
        self.min_budget_applied = 0
        self.insurance_truncation_avoided = 0

        # Top-upç»Ÿè®¡
        self.topup_applied = 0
        self.topup_micro_upgraded = 0
        self.topup_raw_added = 0
        self.topup_tokens_added = 0

        # æ€§èƒ½ç»Ÿè®¡
        self.api_failures = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.concurrent_tasks = 0
        self.embedding_requests = 0
        self.summary_requests = 0

        # å…¶ä»–ç»Ÿè®¡
        self.preserved_messages = 0
        self.processed_messages = 0
        self.summary_messages = 0
        self.emergency_truncations = 0
        self.content_loss_ratio = 0.0
        self.discarded_messages = 0
        self.recovered_messages = 0
        self.window_utilization = 0.0
        self.try_preserve_tokens = 0
        self.try_preserve_messages = 0
        self.try_preserve_summary_messages = 0
        self.keyword_generations = 0
        self.context_maximization_detections = 0
        self.chunk_created = 0
        self.chunk_processed = 0
        self.recursive_summaries = 0
        self.context_max_direct_preserve = 0
        self.context_max_chunked = 0
        self.context_max_summarized = 0
        self.multimodal_extracted = 0
        self.fallback_preserve_applied = 0
        self.user_message_recovery_count = 0
        self.rag_no_results_count = 0
        self.history_message_separation_count = 0
        self.image_processing_errors = 0
        self.syntax_errors_fixed = 0
        self.truncation_skip_count = 0
        self.truncation_recovered_messages = 0
        self.smart_truncation_applied = 0

    def calculate_retention_ratio(self) -> float:
        """è®¡ç®—å†…å®¹ä¿ç•™æ¯”ä¾‹"""
        if self.original_tokens == 0:
            return 0.0
        return self.final_tokens / self.original_tokens

    def calculate_window_usage_ratio(self) -> float:
        """è®¡ç®—å¯¹è¯çª—å£ä½¿ç”¨ç‡"""
        if self.target_tokens == 0:
            return 0.0
        return self.final_tokens / self.target_tokens

    def get_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        retention = self.calculate_retention_ratio()
        window_usage = self.calculate_window_usage_ratio()
        status = "âœ…" if self.zero_loss_guarantee else "âš ï¸"
        summary_lines = [
            "ğŸ“Š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†å®Œæˆ:",
            f"â”œâ”€ æ¶ˆæ¯: {self.original_messages} -> {self.final_messages}æ¡ | tokens: {self.original_tokens:,} -> {self.final_tokens:,}",
            f"â”œâ”€ çª—å£ä½¿ç”¨: {window_usage:.1%} | å†…å®¹ä¿ç•™: {retention:.1%}",
            f"â”œâ”€ Coverage: è¦†ç›–ç‡{self.coverage_rate:.1%}, åŸæ–‡{self.coverage_preserved_count}æ¡+æ‘˜è¦{self.coverage_summary_count}æ¡",
            f"â”œâ”€ æ€§èƒ½: å¤„ç†{self.processing_time:.1f}s, APIè°ƒç”¨{self.summary_requests}æ¬¡, ç¼“å­˜å‘½ä¸­{self.cache_hits}æ¬¡",
            f"â””â”€ ä¸æˆªæ–­: {status}",
        ]
        return "\n".join(summary_lines)


class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨"""

    def __init__(self, event_emitter):
        self.event_emitter = event_emitter
        self.current_step = 0
        self.total_steps = 0
        self.current_phase = ""
        self.phase_progress = 0
        self.phase_total = 0
        self.logged_phases = set()

    def create_progress_bar(self, percentage: float, width: int = 15) -> str:
        """åˆ›å»ºç¾è§‚çš„è¿›åº¦æ¡"""
        filled = int(percentage * width / 100)
        if percentage >= 100:
            bar = "â–ˆ" * width
        else:
            bar = "â–ˆ" * filled + "â–“" * max(0, 1) + "â–‘" * max(0, width - filled - 1)
        return f"[{bar}] {percentage:.1f}%"

    async def start_phase(self, phase_name: str, total_items: int = 0):
        """å¼€å§‹æ–°é˜¶æ®µ"""
        self.current_phase = phase_name
        self.phase_progress = 0
        self.phase_total = total_items
        self.logged_phases.add(phase_name)
        await self.update_status(f"å¼€å§‹ {phase_name}")

    async def update_progress(
        self, completed: int, total: int = None, detail: str = ""
    ):
        """æ›´æ–°è¿›åº¦"""
        if total is None:
            total = self.phase_total
        self.phase_progress = completed

        if total > 0:
            percentage = (completed / total) * 100
            progress_bar = self.create_progress_bar(percentage)
            status = f"{self.current_phase} {progress_bar}"
            if detail:
                status += f" - {detail}"
        else:
            status = f"{self.current_phase}"
            if detail:
                status += f" - {detail}"

        await self.update_status(status, False)

    async def complete_phase(self, message: str = ""):
        """å®Œæˆå½“å‰é˜¶æ®µ"""
        final_message = f"{self.current_phase} å®Œæˆ"
        if message:
            final_message += f" - {message}"
        await self.update_status(final_message, True)

    async def update_status(self, message: str, done: bool = False):
        """æ›´æ–°çŠ¶æ€"""
        if self.event_emitter:
            try:
                message = message.replace("\n", " ").replace("\r", " ")
                message = re.sub(r"\s+", " ", message).strip()
                await self.event_emitter(
                    {
                        "type": "status",
                        "data": {"description": message, "done": done},
                    }
                )
            except Exception as e:
                if str(e) not in self.logged_phases:
                    print(f"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}")
                    self.logged_phases.add(str(e))


class ModelMatcher:
    """æ™ºèƒ½æ¨¡å‹åŒ¹é…å™¨"""

    def __init__(self):
        self.default_limit = 200000
        self.default_image_tokens = 1500

    def _build_model_info(
        self,
        family: str,
        multimodal: bool,
        limit: int,
        image_tokens: int,
        match_type: str,
        matched_pattern: Optional[str] = None,
        hint: Optional[str] = None,
    ) -> Dict[str, Any]:
        result: Dict[str, Any] = {
            "family": family,
            "multimodal": multimodal,
            "limit": limit,
            "image_tokens": image_tokens,
            "match_type": match_type,
        }
        if matched_pattern:
            result["matched_pattern"] = matched_pattern
        if hint:
            result["hint"] = hint
        return result

    def match_model(self, model_name: str) -> Dict[str, Any]:
        """æ™ºèƒ½åŒ¹é…æ¨¡å‹ä¿¡æ¯"""
        if not model_name:
            return self._build_model_info(
                family="unknown",
                multimodal=False,
                limit=self.default_limit,
                image_tokens=self.default_image_tokens,
                match_type="default",
            )

        model_lower = model_name.lower().strip()

        if re.match(r"gpt-5.*", model_lower):
            return self._build_model_info("gpt", True, 200000, 2000, "fuzzy", "gpt-5.*")
        if re.match(r"gpt-4o.*", model_lower):
            return self._build_model_info("gpt", True, 128000, 1500, "fuzzy", "gpt-4o.*")
        if re.match(r"gpt-4.*", model_lower):
            return self._build_model_info("gpt", False, 8192, 0, "fuzzy", "gpt-4.*")
        if re.match(r"claude-4.*", model_lower):
            return self._build_model_info("claude", True, 200000, 1000, "fuzzy", "claude-4.*")
        if re.match(r"claude-3.*", model_lower):
            return self._build_model_info("claude", True, 200000, 1000, "fuzzy", "claude-3.*")
        if re.match(r"doubao.*vision.*", model_lower):
            return self._build_model_info("doubao", True, 128000, 1500, "fuzzy", "doubao.*vision.*")
        if re.match(r"doubao.*", model_lower):
            return self._build_model_info("doubao", False, 50000, 0, "fuzzy", "doubao.*")
        if re.match(r"gemini.*vision.*", model_lower):
            return self._build_model_info("gemini", True, 128000, 800, "fuzzy", "gemini.*vision.*")
        if re.match(r"qwen.*vl.*", model_lower):
            return self._build_model_info("qwen", True, 32000, 1000, "fuzzy", "qwen.*vl.*")

        return self._build_model_info(
            family="unknown",
            multimodal=False,
            limit=self.default_limit,
            image_tokens=self.default_image_tokens,
            match_type="default",
            hint=f"æœªè¯†åˆ«æ¨¡å‹ '{model_name}'ï¼Œå·²ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆ200k / æ–‡æœ¬æ¨¡å‹ï¼‰ã€‚",
        )


class TokenCalculator:
    """Tokenè®¡ç®—å™¨"""

    def __init__(self):
        self._encoding = None
        self.model_info = None

    def set_model_info(self, model_info: dict):
        """è®¾ç½®å½“å‰æ¨¡å‹ä¿¡æ¯"""
        self.model_info = model_info

    def get_encoding(self):
        """è·å–tiktokenç¼–ç å™¨"""
        if not TIKTOKEN_AVAILABLE:
            return None
        if self._encoding is None:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
            except Exception:
                pass
        return self._encoding

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®—"""
        if not text:
            return 0
        encoding = self.get_encoding()
        if encoding:
            try:
                return len(encoding.encode(str(text)))
            except Exception:
                pass
        return len(str(text)) // 4

    def calculate_image_tokens(self, image_data: str) -> int:
        """è®¡ç®—å›¾ç‰‡tokens"""
        if self.model_info:
            return self.model_info.get("image_tokens", 1500)
        return 1500


class InputCleaner:
    """è¾“å…¥æ¸…æ´—ä¸ä¸¥æ ¼å…œåº•"""

    @staticmethod
    def clean_text_for_regex(text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ç”¨äºæ­£åˆ™è¡¨è¾¾å¼"""
        if not text:
            return ""
        try:
            text = text.replace("\u2028", " ").replace("\u2029", " ")
            text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", "", text)
            text = text.replace("\n", " ").replace("\r", " ")
            text = re.sub(r"\s+", " ", text).strip()
            return text
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬æ¸…ç†å¼‚å¸¸: {str(e)[:100]}")
            return "".join(c for c in str(text) if c.isprintable() or c.isspace())[
                :1000
            ]

    @staticmethod
    def validate_and_clean_image_url(image_url: str) -> Tuple[bool, str]:
        """éªŒè¯å¹¶æ¸…æ´—å›¾ç‰‡URL"""
        if not image_url or not isinstance(image_url, str):
            return False, ""

        try:
            image_url = image_url.strip()

            if image_url.startswith(("http://", "https://")):
                return True, image_url

            if image_url.startswith("data:"):
                if "base64," not in image_url:
                    return False, ""
                header, b64 = image_url.split("base64,", 1)
                if not header.lower().startswith("data:image/"):
                    return False, ""
                b64_str = re.sub(r"\s+", "", b64)
                if len(b64_str) < 100:
                    return False, ""
                head = b64_str[:100]
                pad_len = (-len(head)) % 4
                try:
                    base64.b64decode(head + ("=" * pad_len), validate=True)
                except Exception:
                    return False, ""
                return True, f"{header}base64,{b64_str}"

            return False, ""
        except Exception as e:
            print(f"âš ï¸ å›¾ç‰‡URLéªŒè¯å¼‚å¸¸: {str(e)[:100]}")
            return False, ""

    @staticmethod
    def safe_regex_match(pattern: str, text: str) -> bool:
        """å®‰å…¨çš„æ­£åˆ™åŒ¹é…"""
        try:
            cleaned_text = InputCleaner.clean_text_for_regex(text)
            return re.search(pattern, cleaned_text) is not None
        except Exception as e:
            print(f"âš ï¸ æ­£åˆ™åŒ¹é…å¼‚å¸¸: {str(e)[:100]}")
            return False


class MessageChunker:
    """å•æ¡æ¶ˆæ¯å†…åˆ†ç‰‡å¤„ç†å™¨"""

    def __init__(self, token_calculator: TokenCalculator, valves):
        self.token_calculator = token_calculator
        self.valves = valves

    def should_chunk_message(self, message: dict) -> bool:
        """åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦éœ€è¦åˆ†ç‰‡"""
        tokens = self.token_calculator.count_tokens(self.extract_text_content(message))
        return tokens > self.valves.large_message_threshold

    def extract_text_content(self, message: dict) -> str:
        """ä»æ¶ˆæ¯ä¸­æå–æ–‡æœ¬å†…å®¹"""
        content = message.get("content", "")
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
                elif item.get("type") == "image_url":
                    text_parts.append("[å›¾ç‰‡]")
            return " ".join(text_parts)
        else:
            return str(content)

    def chunk_single_message(
        self, message: dict, message_order: MessageOrder
    ) -> List[dict]:
        """å¯¹å•æ¡æ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡å¤„ç†"""
        content_text = self.extract_text_content(message)
        if not self.should_chunk_message(message):
            return [message]

        chunks = self._intelligent_chunk_text(content_text)
        if len(chunks) <= 1:
            return [message]

        chunked_messages = []
        msg_id = message.get("_order_id", "unknown")
        for i, chunk_text in enumerate(chunks):
            chunk_id = message_order.generate_chunk_id(msg_id, i)
            chunk_message = copy.deepcopy(message)
            chunk_message["content"] = chunk_text
            chunk_message["_order_id"] = chunk_id
            chunk_message["_is_chunk"] = True
            chunk_message["_parent_msg_id"] = msg_id
            chunk_message["_chunk_index"] = i
            chunk_message["_total_chunks"] = len(chunks)
            chunked_messages.append(chunk_message)

        return chunked_messages

    def _intelligent_chunk_text(self, text: str) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†ç‰‡"""
        if not text:
            return [text]

        text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", "", text)
        text = text.replace("\u2028", "\n").replace("\u2029", "\n")

        target_size = self.valves.chunk_target_tokens * 4
        min_size = self.valves.chunk_min_tokens * 4
        max_size = self.valves.chunk_max_tokens * 4
        overlap_size = self.valves.chunk_overlap_tokens * 4

        chunks = []
        current_chunk = ""
        paragraphs = re.split(r"\n\s*\n", text)

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            if len(current_chunk) + len(paragraph) > target_size and current_chunk:
                if len(current_chunk) >= min_size:
                    chunks.append(current_chunk.strip())
                    if self.valves.chunk_overlap_tokens > 0:
                        overlap_text = (
                            current_chunk[-overlap_size:]
                            if len(current_chunk) > overlap_size
                            else current_chunk
                        )
                        current_chunk = overlap_text + "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
                else:
                    current_chunk += "\n\n" + paragraph
            else:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph

            if len(current_chunk) > max_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""

        if current_chunk and len(current_chunk.strip()) >= min_size // 2:
            chunks.append(current_chunk.strip())
        elif current_chunk and chunks:
            chunks[-1] += "\n\n" + current_chunk.strip()
        elif current_chunk:
            chunks.append(current_chunk.strip())

        if not chunks and text:
            chunks = [text]

        return chunks

    def preprocess_messages_with_chunking(
        self, messages: List[dict], message_order: MessageOrder
    ) -> List[dict]:
        """é¢„å¤„ç†æ¶ˆæ¯ï¼šå¯¹å¤§æ¶ˆæ¯è¿›è¡Œåˆ†ç‰‡"""
        processed_messages = []
        chunked_count = 0
        for message in messages:
            if self.should_chunk_message(message):
                chunked_messages = self.chunk_single_message(message, message_order)
                processed_messages.extend(chunked_messages)
                if len(chunked_messages) > 1:
                    chunked_count += 1
            else:
                processed_messages.append(message)
        return processed_messages


class CoveragePlanner:
    """Coverageè®¡åˆ’å™¨"""

    def __init__(self, token_calculator: TokenCalculator, valves):
        self.token_calculator = token_calculator
        self.valves = valves

    def plan_adaptive_coverage_summaries(
        self, scored_msgs: List[dict], total_budget: int
    ) -> Tuple[List[dict], int]:
        """è§„åˆ’è‡ªé€‚åº”è¦†ç›–æ‘˜è¦"""
        if not scored_msgs:
            return [], 0

        HIGH, MID, LOW = self._classify_messages_by_score(scored_msgs)
        adaptive_blocks = self._create_adaptive_blocks(LOW)
        entries, ideal_total_cost = self._calculate_ideal_budgets(
            HIGH, MID, adaptive_blocks
        )

        if ideal_total_cost > total_budget:
            entries, actual_cost = self._apply_proportional_scaling(
                entries, total_budget
            )
        else:
            entries, actual_cost = self._apply_upward_expansion(
                entries, total_budget, ideal_total_cost
            )

        if actual_cost > total_budget * 1.1:
            entries, actual_cost = self._apply_extreme_fallback(
                scored_msgs, total_budget
            )

        return entries, actual_cost

    def _classify_messages_by_score(
        self, scored_msgs: List[dict]
    ) -> Tuple[List[dict], List[dict], List[dict]]:
        """æŒ‰åˆ†æ•°åˆ†æ¡£æ¶ˆæ¯"""
        HIGH, MID, LOW = [], [], []
        for item in scored_msgs:
            if item["score"] >= self.valves.coverage_high_score_threshold:
                HIGH.append(item)
            elif item["score"] >= self.valves.coverage_mid_score_threshold:
                MID.append(item)
            else:
                LOW.append(item)
        return HIGH, MID, LOW

    def _create_adaptive_blocks(self, low_messages: List[dict]) -> List[dict]:
        """æŒ‰åŸæ–‡tokené‡è‡ªé€‚åº”åˆ†å—"""
        if not low_messages:
            return []

        low_sorted = sorted(low_messages, key=lambda x: x["idx"])
        blocks = []
        current_block = []
        current_tokens = 0
        raw_block_target = self.valves.raw_block_target

        for item in low_sorted:
            msg_tokens = item["tokens"]
            should_cut_block = False

            if current_tokens + msg_tokens > raw_block_target and current_block:
                should_cut_block = True

            if current_block and abs(item["idx"] - current_block[-1]["idx"]) > 5:
                should_cut_block = True

            if current_block:
                prev_role = current_block[-1]["msg"].get("role", "")
                curr_role = item["msg"].get("role", "")
                if (
                    prev_role != curr_role
                    and prev_role in ["user", "assistant"]
                    and curr_role in ["user", "assistant"]
                ):
                    should_cut_block = True

            if current_block:
                score_diff = abs(item["score"] - current_block[-1]["score"])
                if score_diff > 0.3:
                    should_cut_block = True

            if should_cut_block:
                if current_block:
                    blocks.append(
                        {
                            "type": "adaptive_block",
                            "idx_range": (
                                current_block[0]["idx"],
                                current_block[-1]["idx"],
                            ),
                            "msgs": [item["msg"] for item in current_block],
                            "raw_tokens": current_tokens,
                            "avg_score": sum(item["score"] for item in current_block)
                            / len(current_block),
                            "msg_count": len(current_block),
                        }
                    )
                current_block = [item]
                current_tokens = msg_tokens
            else:
                current_block.append(item)
                current_tokens += msg_tokens

        if current_block:
            blocks.append(
                {
                    "type": "adaptive_block",
                    "idx_range": (current_block[0]["idx"], current_block[-1]["idx"]),
                    "msgs": [item["msg"] for item in current_block],
                    "raw_tokens": current_tokens,
                    "avg_score": sum(item["score"] for item in current_block)
                    / len(current_block),
                    "msg_count": len(current_block),
                }
            )

        if len(blocks) > self.valves.max_blocks:
            blocks = self._merge_small_blocks(blocks)

        return blocks

    def _merge_small_blocks(self, blocks: List[dict]) -> List[dict]:
        """åˆå¹¶å°å—"""
        if len(blocks) <= self.valves.max_blocks:
            return blocks

        blocks.sort(key=lambda x: x["raw_tokens"])
        merged_blocks = []
        i = 0

        while i < len(blocks):
            current_block = blocks[i]
            if (
                i + 1 < len(blocks)
                and len(merged_blocks) + (len(blocks) - i) > self.valves.max_blocks
            ):
                next_block = blocks[i + 1]
                if (
                    current_block["raw_tokens"] + next_block["raw_tokens"]
                    <= self.valves.raw_block_target * 2
                ):
                    merged_block = {
                        "type": "adaptive_block",
                        "idx_range": (
                            current_block["idx_range"][0],
                            next_block["idx_range"][1],
                        ),
                        "msgs": current_block["msgs"] + next_block["msgs"],
                        "raw_tokens": current_block["raw_tokens"]
                        + next_block["raw_tokens"],
                        "avg_score": (
                            current_block["avg_score"] * current_block["msg_count"]
                            + next_block["avg_score"] * next_block["msg_count"]
                        )
                        / (current_block["msg_count"] + next_block["msg_count"]),
                        "msg_count": current_block["msg_count"]
                        + next_block["msg_count"],
                    }
                    merged_blocks.append(merged_block)
                    i += 2
                    continue
            merged_blocks.append(current_block)
            i += 1

        return merged_blocks

    def _calculate_ideal_budgets(
        self, high_msgs: List[dict], mid_msgs: List[dict], adaptive_blocks: List[dict]
    ) -> Tuple[List[dict], int]:
        """è®¡ç®—ç†æƒ³é¢„ç®—éœ€æ±‚"""
        entries = []
        total_cost = 0

        for grp, per_token in [
            (high_msgs, self.valves.coverage_high_summary_tokens),
            (mid_msgs, self.valves.coverage_mid_summary_tokens),
        ]:
            for item in grp:
                msg_id = item["msg"].get("_order_id", f"msg_{item['idx']}")
                entry = {
                    "type": "micro",
                    "msg_id": msg_id,
                    "ideal_budget": per_token,
                    "floor_budget": max(self.valves.min_summary_tokens, per_token // 3),
                    "msg": item["msg"],
                    "score": item["score"],
                }
                entries.append(entry)
                total_cost += per_token

        for block in adaptive_blocks:
            floor_budget = max(
                self.valves.min_block_summary_tokens, self.valves.floor_block
            )
            size_factor = min(3.0, block["raw_tokens"] / self.valves.raw_block_target)
            ideal_budget = int(
                floor_budget
                + (self.valves.coverage_block_summary_tokens - floor_budget)
                * size_factor
            )
            block_key = f"block_{block['idx_range'][0]}_{block['idx_range'][1]}"
            entry = {
                "type": "adaptive_block",
                "block_key": block_key,
                "idx_range": block["idx_range"],
                "ideal_budget": ideal_budget,
                "floor_budget": floor_budget,
                "msgs": block["msgs"],
                "raw_tokens": block["raw_tokens"],
                "avg_score": block["avg_score"],
            }
            entries.append(entry)
            total_cost += ideal_budget

        return entries, total_cost

    def _apply_proportional_scaling(
        self, entries: List[dict], available_budget: int
    ) -> Tuple[List[dict], int]:
        """ä¸€æ¬¡æ€§æ¯”ä¾‹ç¼©æ”¾"""
        total_floors = sum(entry["floor_budget"] for entry in entries)
        total_ideals = sum(entry["ideal_budget"] for entry in entries)

        if total_floors > available_budget:
            return self._apply_extreme_fallback_from_entries(entries, available_budget)

        available_for_scaling = available_budget - total_floors
        scalable_amount = total_ideals - total_floors

        if scalable_amount <= 0:
            alpha = 0
        else:
            alpha = available_for_scaling / scalable_amount
            alpha = min(1.0, alpha)

        total_assigned = 0
        for entry in entries:
            floor_budget = entry["floor_budget"]
            ideal_budget = entry["ideal_budget"]
            scaled_budget = floor_budget + alpha * (ideal_budget - floor_budget)
            entry["budget"] = int(round(scaled_budget))
            total_assigned += entry["budget"]

        error = available_budget - total_assigned
        if error != 0:
            scored_entries = [
                (entry.get("score", entry.get("avg_score", 0)), entry)
                for entry in entries
            ]
            scored_entries.sort(key=lambda x: x[0], reverse=True)

            if error > 0:
                for _, entry in scored_entries:
                    if error <= 0:
                        break
                    entry["budget"] += 1
                    error -= 1
            else:
                for _, entry in reversed(scored_entries):
                    if error >= 0:
                        break
                    if entry["budget"] > entry["floor_budget"]:
                        entry["budget"] -= 1
                        error += 1

        final_cost = sum(entry["budget"] for entry in entries)
        return entries, final_cost

    def _apply_upward_expansion(
        self, entries: List[dict], available_budget: int, ideal_total_cost: int
    ) -> Tuple[List[dict], int]:
        """å‘ä¸Šæ‰©å¼ æ¨¡å¼"""
        expansion_cap = 3.0
        target_usage = self.valves.target_window_usage
        target_cost = int(available_budget * target_usage)

        if ideal_total_cost >= target_cost:
            for entry in entries:
                entry["budget"] = entry["ideal_budget"]
            return entries, ideal_total_cost

        expansion_factor = min(expansion_cap, target_cost / ideal_total_cost)
        total_assigned = 0

        for entry in entries:
            base_budget = entry["ideal_budget"]
            if entry["type"] == "adaptive_block":
                expanded_budget = int(base_budget * expansion_factor)
            elif (
                entry["type"] == "micro"
                and entry.get("score", 0) >= self.valves.coverage_high_score_threshold
            ):
                expanded_budget = int(base_budget * min(2.0, expansion_factor))
            else:
                expanded_budget = base_budget
            entry["budget"] = expanded_budget
            total_assigned += expanded_budget

        if total_assigned > available_budget:
            scale_down = available_budget / total_assigned
            for entry in entries:
                entry["budget"] = int(entry["budget"] * scale_down)
            total_assigned = sum(entry["budget"] for entry in entries)

        return entries, total_assigned

    def _apply_extreme_fallback(
        self, scored_msgs: List[dict], available_budget: int
    ) -> Tuple[List[dict], int]:
        """æç«¯é€€åŒ–ï¼šå•æ¡å…¨å±€å—æ‘˜è¦"""
        global_budget = max(
            self.valves.min_block_summary_tokens, int(available_budget * 0.9)
        )
        sorted_msgs = sorted(scored_msgs, key=lambda x: x["idx"])
        all_msgs = [item["msg"] for item in sorted_msgs]
        entry = {
            "type": "global_block",
            "block_key": f"global_0_{len(sorted_msgs)-1}",
            "idx_range": (0, len(sorted_msgs) - 1),
            "budget": global_budget,
            "msgs": all_msgs,
            "avg_score": sum(item["score"] for item in sorted_msgs) / len(sorted_msgs),
        }
        return [entry], global_budget

    def _apply_extreme_fallback_from_entries(
        self, entries: List[dict], available_budget: int
    ) -> Tuple[List[dict], int]:
        """ä»ç°æœ‰æ¡ç›®æ‰§è¡Œæç«¯é€€åŒ–"""
        all_msgs = []
        for entry in entries:
            if entry["type"] == "micro":
                all_msgs.append(entry["msg"])
            elif entry["type"] == "adaptive_block":
                all_msgs.extend(entry["msgs"])

        all_msgs.sort(key=lambda x: x.get("_original_index", 0))
        global_budget = max(
            self.valves.min_block_summary_tokens, int(available_budget * 0.9)
        )
        entry = {
            "type": "global_block",
            "block_key": f"global_0_{len(all_msgs)-1}",
            "idx_range": (0, len(all_msgs) - 1),
            "budget": global_budget,
            "msgs": all_msgs,
            "avg_score": 0.5,
        }
        return [entry], global_budget


# ========== ä¸»è¿‡æ»¤å™¨ç±» ==========


class Filter:
    class Valves(BaseModel):
        # ========== Auto Memoryé…ç½® ==========
        enable_auto_memory: bool = Field(
            default=True, description="ğŸ§  å¯ç”¨è‡ªåŠ¨è®°å¿†ç®¡ç†"
        )
        memory_messages_to_consider: int = Field(
            default=4, description="ğŸ§  è®°å¿†æå–è€ƒè™‘çš„æ¶ˆæ¯æ•°"
        )
        memory_related_memories_n: int = Field(
            default=5, description="ğŸ§  ç›¸å…³è®°å¿†æ£€ç´¢æ•°é‡"
        )
        memory_minimum_similarity: Optional[float] = Field(
            default=0.0, description="ğŸ§  è®°å¿†æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        memory_force_add_prefixes: str = Field(
            default="è®°ä½:;remember:",
            description="ğŸ§  å¼ºåˆ¶å†™å…¥è®°å¿†å‰ç¼€ï¼ˆç”¨ ; åˆ†éš”ï¼‰ï¼Œå‘½ä¸­åˆ™ç›´æ¥ addï¼Œä¸ç»è¿‡LLMï¼Œä¾‹å¦‚ï¼šè®°ä½:;remember:",
        )
        override_memory_context: bool = Field(
            default=False, description="ğŸ§  æ‹¦æˆªå¹¶è¦†ç›–è®°å¿†ä¸Šä¸‹æ–‡æ³¨å…¥"
        )
        memory_show_status: bool = Field(
            default=False,
            description="ğŸ§ ï¼ˆå·²åºŸå¼ƒï¼‰å‰å°ä¸æ˜¾ç¤ºè‡ªåŠ¨è®°å¿†çŠ¶æ€ï¼ˆä¿ç•™å­—æ®µä»…ä¸ºå…¼å®¹æ—§é…ç½®ï¼‰",
        )

        # ========== åŸºç¡€æ§åˆ¶ ==========
        enable_processing: bool = Field(
            default=True, description="ğŸ”„ å¯ç”¨å†…å®¹æœ€å¤§åŒ–å¤„ç†"
        )
        excluded_models: str = Field(
            default="", description="ğŸš« æ’é™¤æ¨¡å‹åˆ—è¡¨(é€—å·åˆ†éš”)"
        )
        suppress_frontend_when_idle: bool = Field(
            default=True, description="ğŸ•¶ï¸ æ— éœ€å¤„ç†æ—¶ä¸æ˜¾ç¤ºä»»ä½•å‰ç«¯è¿›åº¦/æ—¥å¿—"
        )
        enable_window_topup: bool = Field(
            default=False, description="ğŸ§¯ ä»…åœ¨è¶…é™å‹ç¼©åæ‰å…è®¸çª—å£å¡«å……"
        )

        # ========== æ ¸å¿ƒé…ç½® ==========
        max_window_utilization: float = Field(
            default=0.95, description="ğŸªŸ æœ€å¤§çª—å£åˆ©ç”¨ç‡(95%)"
        )
        aggressive_content_recovery: bool = Field(
            default=True, description="ğŸ”„ æ¿€è¿›å†…å®¹åˆå¹¶æ¨¡å¼"
        )
        min_preserve_ratio: float = Field(
            default=0.75, description="ğŸ”’ æœ€å°å†…å®¹ä¿ç•™æ¯”ä¾‹(75%)"
        )

        # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç­–ç•¥é…ç½® ==========
        enable_coverage_first: bool = Field(
            default=True, description="ğŸ¯ å¯ç”¨ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç­–ç•¥"
        )
        coverage_high_score_threshold: float = Field(
            default=0.7, description="ğŸ¯ é«˜æƒé‡é˜ˆå€¼(70%)"
        )
        coverage_mid_score_threshold: float = Field(
            default=0.4, description="ğŸ¯ ä¸­æƒé‡é˜ˆå€¼(40%)"
        )
        coverage_high_summary_tokens: int = Field(
            default=100, description="ğŸ“„ é«˜æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_mid_summary_tokens: int = Field(
            default=50, description="ğŸ“„ ä¸­æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_low_summary_tokens: int = Field(
            default=20, description="ğŸ“„ ä½æƒé‡æ¶ˆæ¯å¾®æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_block_summary_tokens: int = Field(
            default=350, description="ğŸ“š å—æ‘˜è¦ç›®æ ‡tokens"
        )
        coverage_upgrade_ratio: float = Field(
            default=0.3, description="â¬†ï¸ å‡çº§é¢„ç®—æ¯”ä¾‹(30%)"
        )

        # ========== è‡ªé€‚åº”åˆ†å—é…ç½® ==========
        raw_block_target: int = Field(
            default=15000, description="ğŸ§© è‡ªé€‚åº”å—ç›®æ ‡åŸæ–‡tokens"
        )
        floor_block: int = Field(default=300, description="ğŸ“ å—æ‘˜è¦æœ€å°é¢„ç®—tokens")
        max_blocks: int = Field(default=8, description="ğŸ“š æœ€å¤§å—æ•°é‡")
        upgrade_min_pct: float = Field(
            default=0.2, description="â¬†ï¸ å‡çº§æ± æœ€å°é¢„ç•™æ¯”ä¾‹(20%)"
        )

        # ========== ä¸æˆªæ–­ä¿éšœé…ç½® ==========
        enable_zero_loss_guarantee: bool = Field(
            default=True, description="ğŸ›¡ï¸ å¯ç”¨ä¸æˆªæ–­ä¿éšœ"
        )
        min_summary_tokens: int = Field(
            default=30, description="ğŸ“ æœ€å°å¾®æ‘˜è¦tokens(ä¿åº•)"
        )
        min_block_summary_tokens: int = Field(
            default=200, description="ğŸ“ æœ€å°å—æ‘˜è¦tokens(ä¿åº•)"
        )
        max_budget_adjustment_rounds: int = Field(
            default=5, description="ğŸ”§ æœ€å¤§é¢„ç®—è°ƒæ•´è½®æ¬¡"
        )
        disable_insurance_truncation: bool = Field(
            default=True, description="ğŸš« ç¦ç”¨ä¿é™©æˆªæ–­(å¼ºåˆ¶ä¸æˆªæ–­)"
        )

        # ========== å°½é‡ä¿ç•™é…ç½® ==========
        enable_try_preserve: bool = Field(
            default=True, description="ğŸ”’ å¯ç”¨å°½é‡ä¿ç•™æœºåˆ¶"
        )
        try_preserve_ratio: float = Field(
            default=0.40, description="ğŸ”’ å°½é‡ä¿ç•™é¢„ç®—æ¯”ä¾‹(40%)"
        )
        try_preserve_exchanges: int = Field(
            default=3, description="ğŸ”’ å°½é‡ä¿ç•™å¯¹è¯è½®æ¬¡æ•°"
        )

        # ========== å“åº”ç©ºé—´é…ç½® ==========
        response_buffer_ratio: float = Field(
            default=0.06, description="ğŸ“ å“åº”ç©ºé—´é¢„ç•™æ¯”ä¾‹(6%)"
        )
        response_buffer_max: int = Field(
            default=3000, description="ğŸ“ å“åº”ç©ºé—´æœ€å¤§å€¼(tokens)"
        )
        response_buffer_min: int = Field(
            default=1000, description="ğŸ“ å“åº”ç©ºé—´æœ€å°å€¼(tokens)"
        )

        # ========== å¤šæ¨¡æ€å¤„ç†é…ç½® ==========
        multimodal_direct_threshold: float = Field(
            default=0.70, description="ğŸ¯ å¤šæ¨¡æ€ç›´æ¥è¾“å…¥Tokené¢„ç®—é˜ˆå€¼(70%)"
        )
        preserve_images_in_multimodal: bool = Field(
            default=True, description="ğŸ“¸ å¤šæ¨¡æ€æ¨¡å‹æ˜¯å¦ä¿ç•™åŸå§‹å›¾ç‰‡"
        )
        always_process_images_before_summary: bool = Field(
            default=True, description="ğŸ“ æ‘˜è¦å‰æ€»æ˜¯å…ˆå¤„ç†å›¾ç‰‡"
        )

        # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†é…ç½® ==========
        enable_context_maximization: bool = Field(
            default=True, description="ğŸ“š å¯ç”¨ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†"
        )
        context_max_direct_preserve_ratio: float = Field(
            default=0.40, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç›´æ¥ä¿ç•™æ¯”ä¾‹(40%)"
        )
        context_max_processing_ratio: float = Field(
            default=0.45, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†é¢„ç®—æ¯”ä¾‹(45%)"
        )
        context_max_fallback_ratio: float = Field(
            default=0.15, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å®¹é”™é¢„ç®—æ¯”ä¾‹(15%)"
        )
        context_max_skip_rag: bool = Field(
            default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–è·³è¿‡RAGå¤„ç†"
        )
        context_max_prioritize_recent: bool = Field(
            default=True, description="ğŸ“š ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¼˜å…ˆä¿ç•™æœ€è¿‘å†…å®¹"
        )

        # ========== å®¹é”™æœºåˆ¶é…ç½® ==========
        enable_fallback_preservation: bool = Field(
            default=True, description="ğŸ›¡ï¸ å¯ç”¨å®¹é”™ä¿æŠ¤æœºåˆ¶"
        )
        fallback_preserve_ratio: float = Field(
            default=0.25, description="ğŸ›¡ï¸ å®¹é”™ä¿æŠ¤é¢„ç•™æ¯”ä¾‹(25%)"
        )
        min_history_messages: int = Field(default=8, description="ğŸ›¡ï¸ æœ€å°‘å†å²æ¶ˆæ¯æ•°é‡")
        force_preserve_recent_user_exchanges: int = Field(
            default=3, description="ğŸ›¡ï¸ å¼ºåˆ¶ä¿ç•™æœ€è¿‘ç”¨æˆ·å¯¹è¯è½®æ¬¡"
        )

        # ========== åŠŸèƒ½å¼€å…³ ==========
        enable_multimodal: bool = Field(default=True, description="ğŸ–¼ï¸ å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
        enable_vision_preprocessing: bool = Field(
            default=True, description="ğŸ‘ï¸ å¯ç”¨å›¾ç‰‡é¢„å¤„ç†"
        )
        enable_vector_retrieval: bool = Field(
            default=True, description="ğŸ” å¯ç”¨å‘é‡æ£€ç´¢"
        )
        enable_intelligent_chunking: bool = Field(
            default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡"
        )
        enable_recursive_summarization: bool = Field(
            default=True, description="ğŸ”„ å¯ç”¨é€’å½’æ‘˜è¦"
        )
        enable_reranking: bool = Field(default=True, description="ğŸ”„ å¯ç”¨é‡æ’åº")

        # ========== æ™ºèƒ½å…³é”®å­—ç”Ÿæˆå’Œä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ ==========
        enable_keyword_generation: bool = Field(
            default=True, description="ğŸ”‘ å¯ç”¨æ™ºèƒ½å…³é”®å­—ç”Ÿæˆ"
        )
        enable_ai_context_max_detection: bool = Field(
            default=True, description="ğŸ§  å¯ç”¨AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹"
        )
        keyword_generation_for_context_max: bool = Field(
            default=True, description="ğŸ”‘ å¯¹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¯ç”¨å…³é”®å­—ç”Ÿæˆ"
        )

        # ========== ç»Ÿè®¡å’Œè°ƒè¯• ==========
        enable_detailed_stats: bool = Field(default=True, description="ğŸ“Š å¯ç”¨è¯¦ç»†ç»Ÿè®¡")
        enable_detailed_progress: bool = Field(
            default=True, description="ğŸ“± å¯ç”¨è¯¦ç»†è¿›åº¦æ˜¾ç¤º"
        )
        debug_level: int = Field(default=0, description="ğŸ› è°ƒè¯•çº§åˆ« 0-3")
        show_frontend_progress: bool = Field(
            default=True, description="ğŸ“± æ˜¾ç¤ºå¤„ç†è¿›åº¦"
        )

        # ========== APIé…ç½® ==========
        api_error_retry_times: int = Field(default=2, description="ğŸ”„ APIé”™è¯¯é‡è¯•æ¬¡æ•°")
        api_error_retry_delay: float = Field(
            default=1.0, description="â±ï¸ APIé”™è¯¯é‡è¯•å»¶è¿Ÿ(ç§’)"
        )

        # ========== Tokenç®¡ç† ==========
        default_token_limit: int = Field(default=200000, description="âš–ï¸ é»˜è®¤tokené™åˆ¶")
        token_safety_ratio: float = Field(
            default=0.92, description="ğŸ›¡ï¸ Tokenå®‰å…¨æ¯”ä¾‹(92%)"
        )
        target_window_usage: float = Field(
            default=0.85, description="ğŸªŸ ç›®æ ‡çª—å£ä½¿ç”¨ç‡(85%)"
        )
        max_processing_iterations: int = Field(
            default=5, description="ğŸ”„ æœ€å¤§å¤„ç†è¿­ä»£æ¬¡æ•°"
        )

        # ========== ä¿æŠ¤ç­–ç•¥ ==========
        force_preserve_current_user_message: bool = Field(
            default=True, description="ğŸ”’ å¼ºåˆ¶ä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯(æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯)"
        )
        preserve_recent_exchanges: int = Field(
            default=4, description="ğŸ’¬ ä¿æŠ¤æœ€è¿‘å®Œæ•´å¯¹è¯è½®æ¬¡"
        )
        max_preserve_ratio: float = Field(
            default=0.3, description="ğŸ”’ ä¿æŠ¤æ¶ˆæ¯æœ€å¤§tokenæ¯”ä¾‹"
        )
        max_single_message_tokens: int = Field(
            default=20000, description="ğŸ“ å•æ¡æ¶ˆæ¯æœ€å¤§token"
        )

        # ========== æ™ºèƒ½åˆ†ç‰‡é…ç½® ==========
        enable_smart_chunking: bool = Field(default=True, description="ğŸ§© å¯ç”¨æ™ºèƒ½åˆ†ç‰‡")
        chunk_target_tokens: int = Field(default=4000, description="ğŸ§© åˆ†ç‰‡ç›®æ ‡tokenæ•°")
        chunk_overlap_tokens: int = Field(default=300, description="ğŸ”— åˆ†ç‰‡é‡å tokenæ•°")
        chunk_min_tokens: int = Field(default=1000, description="ğŸ“ åˆ†ç‰‡æœ€å°tokenæ•°")
        chunk_max_tokens: int = Field(default=4000, description="ğŸ“ åˆ†ç‰‡æœ€å¤§tokenæ•°")
        large_message_threshold: int = Field(
            default=10000, description="ğŸ“ å¤§æ¶ˆæ¯åˆ†ç‰‡é˜ˆå€¼"
        )
        preserve_paragraph_integrity: bool = Field(
            default=True, description="ğŸ“ ä¿æŒæ®µè½å®Œæ•´æ€§"
        )
        preserve_sentence_integrity: bool = Field(
            default=True, description="ğŸ“ ä¿æŒå¥å­å®Œæ•´æ€§"
        )
        preserve_code_blocks: bool = Field(
            default=True, description="ğŸ’» ä¿æŒä»£ç å—å®Œæ•´æ€§"
        )

        # ========== å†…å®¹ä¼˜å…ˆçº§è®¾ç½® ==========
        high_priority_content: str = Field(
            default="ä»£ç ,é…ç½®,å‚æ•°,æ•°æ®,é”™è¯¯,è§£å†³æ–¹æ¡ˆ,æ­¥éª¤,æ–¹æ³•,æŠ€æœ¯ç»†èŠ‚,API,å‡½æ•°,ç±»,å˜é‡,é—®é¢˜,bug,ä¿®å¤,å®ç°,ç®—æ³•,æ¶æ„,ç”¨æˆ·é—®é¢˜,å…³é”®å›ç­”",
            description="ğŸ¯ é«˜ä¼˜å…ˆçº§å†…å®¹å…³é”®è¯(é€—å·åˆ†éš”)",
        )

        # ========== ç»Ÿä¸€çš„APIé…ç½® ==========
        api_base: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ”— APIåŸºç¡€åœ°å€",
        )
        api_key: str = Field(default="", description="ğŸ”‘ APIå¯†é’¥")

        # ========== å¤šæ¨¡æ€æ¨¡å‹é…ç½® ==========
        multimodal_model: str = Field(
            default="doubao-1.5-vision-pro-250328", description="ğŸ–¼ï¸ å¤šæ¨¡æ€æ¨¡å‹"
        )

        # ========== æ–‡æœ¬æ¨¡å‹é…ç½® ==========
        text_model: str = Field(
            default="doubao-1-5-lite-32k-250115", description="ğŸ“ æ–‡æœ¬å¤„ç†æ¨¡å‹"
        )

        # ========== è®°å¿†ç®¡ç†æ¨¡å‹é…ç½® ==========
        memory_model: str = Field(
            default="doubao-1-5-lite-32k-250115", description="ğŸ§  è®°å¿†ç®¡ç†æ¨¡å‹"
        )

        # ========== å‘é‡æ¨¡å‹é…ç½® ==========
        text_vector_model: str = Field(
            default="doubao-embedding-large-text-250515", description="ğŸ§  æ–‡æœ¬å‘é‡æ¨¡å‹"
        )
        multimodal_vector_model: str = Field(
            default="doubao-embedding-vision-250615", description="ğŸ§  å¤šæ¨¡æ€å‘é‡æ¨¡å‹"
        )

        # ========== Visionç›¸å…³é…ç½® ==========
        vision_prompt_template: str = Field(
            default="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€æ–‡å­—ã€é¢œè‰²ã€å¸ƒå±€ç­‰æ‰€æœ‰å¯è§ä¿¡æ¯ã€‚ç‰¹åˆ«æ³¨æ„ä»£ç ã€é…ç½®ã€æ•°æ®ç­‰æŠ€æœ¯ä¿¡æ¯ã€‚ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚å¦‚æœå›¾ç‰‡åŒ…å«æ–‡å­—å†…å®¹ï¼Œè¯·å®Œæ•´è½¬å½•å‡ºæ¥ã€‚",
            description="ğŸ‘ï¸ Visionæç¤ºè¯",
        )
        vision_max_tokens: int = Field(
            default=2500, description="ğŸ‘ï¸ Visionæœ€å¤§è¾“å‡ºtokens"
        )

        # ========== å…³é”®å­—ç”Ÿæˆé…ç½® ==========
        keyword_generation_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æœç´¢å…³é”®å­—ç”ŸæˆåŠ©æ‰‹ã€‚ç”¨æˆ·è¾“å…¥äº†ä¸€ä¸ªæŸ¥è¯¢ï¼Œä½ éœ€è¦ç”Ÿæˆå¤šä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—æ¥å¸®åŠ©åœ¨å¯¹è¯å†å²ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚
ğŸ“‹ ä»»åŠ¡è¦æ±‚ï¼š
1. åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œä¸»é¢˜
2. ç”Ÿæˆ5-10ä¸ªç›¸å…³çš„æœç´¢å…³é”®å­—
3. åŒ…å«åŒä¹‰è¯ã€ç›¸å…³è¯ã€æŠ€æœ¯æœ¯è¯­
4. å¯¹äºå®½æ³›æŸ¥è¯¢ï¼ˆå¦‚"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ï¼‰ï¼Œç”Ÿæˆé€šç”¨ä½†æœ‰æ•ˆçš„å…³é”®å­—
5. å…³é”®å­—åº”è¯¥èƒ½è¦†ç›–å¯èƒ½çš„å¯¹è¯ä¸»é¢˜
ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
ç›´æ¥è¾“å‡ºå…³é”®å­—ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
ç°åœ¨è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆå…³é”®å­—ï¼š""",
            description="ğŸ”‘ å…³é”®å­—ç”Ÿæˆæç¤ºè¯",
        )

        # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹é…ç½® ==========
        context_max_detection_prompt: str = Field(
            default="""ä½ æ˜¯ä¸“ä¸šçš„æŸ¥è¯¢æ„å›¾åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ã€‚
ğŸ“‹ åˆ¤æ–­æ ‡å‡†ï¼š
éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- è¯¢é—®"èŠäº†ä»€ä¹ˆ"ã€"è¯´äº†ä»€ä¹ˆ"ã€"è®¨è®ºäº†ä»€ä¹ˆ"ç­‰å®½æ³›å†…å®¹
- è¯¢é—®"ä¹‹å‰çš„å†…å®¹"ã€"å†å²è®°å½•"ã€"å¯¹è¯å†å²"ç­‰
- ç¼ºä¹å…·ä½“çš„ä¸»é¢˜ã€å…³é”®è¯æˆ–æ˜ç¡®çš„æœç´¢æ„å›¾
- æŸ¥è¯¢è¯æ±‡å°‘äº3ä¸ªæœ‰æ•ˆè¯æ±‡
ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–çš„æŸ¥è¯¢ç‰¹å¾ï¼š
- åŒ…å«æ˜ç¡®çš„ä¸»é¢˜ã€æŠ€æœ¯æœ¯è¯­ã€äº§å“åç§°ç­‰
- æœ‰å…·ä½“çš„é—®é¢˜æŒ‡å‘
- åŒ…å«è¯¦ç»†çš„æè¿°æˆ–èƒŒæ™¯ä¿¡æ¯
ğŸ“ è¾“å‡ºæ ¼å¼ï¼š
åªè¾“å‡º "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" æˆ– "ä¸éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
ç°åœ¨è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢ï¼š""",
            description="ğŸ§  ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹æç¤ºè¯",
        )

        # ========== å‘é‡æ£€ç´¢é…ç½® ==========
        vector_similarity_threshold: float = Field(
            default=0.06, description="ğŸ¯ åŸºç¡€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        multimodal_similarity_threshold: float = Field(
            default=0.04, description="ğŸ–¼ï¸ å¤šæ¨¡æ€ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        text_similarity_threshold: float = Field(
            default=0.08, description="ğŸ“ æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        vector_top_k: int = Field(default=150, description="ğŸ” å‘é‡æ£€ç´¢Top-Kæ•°é‡")

        # ========== é‡æ’åºAPIé…ç½® ==========
        rerank_api_base: str = Field(
            default="https://api.bochaai.com", description="ğŸ”„ é‡æ’åºAPI"
        )
        rerank_api_key: str = Field(default="", description="ğŸ”‘ é‡æ’åºå¯†é’¥")
        rerank_model: str = Field(default="gte-rerank", description="ğŸ§  é‡æ’åºæ¨¡å‹")
        rerank_top_k: int = Field(default=100, description="ğŸ” é‡æ’åºè¿”å›æ•°é‡")

        # ========== æ‘˜è¦é…ç½® ==========
        max_summary_length: int = Field(default=25000, description="ğŸ“ æ‘˜è¦æœ€å¤§é•¿åº¦")
        min_summary_ratio: float = Field(
            default=0.30, description="ğŸ“ æ‘˜è¦æœ€å°é•¿åº¦æ¯”ä¾‹"
        )
        summary_compression_ratio: float = Field(
            default=0.40, description="ğŸ“Š æ‘˜è¦å‹ç¼©æ¯”ä¾‹"
        )
        max_recursion_depth: int = Field(default=3, description="ğŸ”„ æœ€å¤§é€’å½’æ·±åº¦")

        # ========== æ€§èƒ½é…ç½® ==========
        max_concurrent_requests: int = Field(default=6, description="âš¡ æœ€å¤§å¹¶å‘æ•°")
        request_timeout: int = Field(default=90, description="â±ï¸ è¯·æ±‚è¶…æ—¶(ç§’)")

        # ========== ç¼“å­˜é…ç½® ==========
        enable_embedding_cache: bool = Field(
            default=True, description="ğŸ’¾ å¯ç”¨å‘é‡ç¼“å­˜"
        )
        cache_max_size: int = Field(default=1000, description="ğŸ’¾ ç¼“å­˜æœ€å¤§æ¡æ•°")

    def __init__(self):
        print("ğŸ“ é«˜çº§ä¸Šä¸‹æ–‡ç®¡ç†å™¨ + è‡ªåŠ¨è®°å¿† åˆå§‹åŒ–ä¸­...")
        self.valves = self.Valves()

        # Auto Memory æ—¥å¿—
        self.logger = logging.getLogger(__name__ + ".auto_memory")

        # åˆå§‹åŒ–åŸæœ‰ç»„ä»¶
        self.model_matcher = ModelMatcher()
        self.token_calculator = TokenCalculator()
        self.input_cleaner = InputCleaner()
        self.message_chunker = MessageChunker(self.token_calculator, self.valves)
        self.coverage_planner = CoveragePlanner(self.token_calculator, self.valves)

        # åˆå§‹åŒ–ç¼“å­˜
        if self.valves.enable_embedding_cache:
            self.embedding_cache = EmbeddingCache(self.valves.cache_max_size)
        else:
            self.embedding_cache = None

        # å¤„ç†ç»Ÿè®¡
        self.stats = ProcessingStats()

        # æ¶ˆæ¯é¡ºåºç®¡ç†å™¨
        self.message_order = None
        self.current_processing_id = None
        self.current_user_message = None
        self.current_model_info = None
        self.model_runtime_overrides: Dict[str, Dict[str, Any]] = {}

        # Auto Memoryç›¸å…³
        self.current_user_obj = None

        # è§£æé…ç½®
        self._parse_configurations()
        print("âœ… åˆå§‹åŒ–å®Œæˆ - é«˜çº§ä¸Šä¸‹æ–‡ç®¡ç†å™¨ + è‡ªåŠ¨è®°å¿†")

    def _parse_configurations(self):
        """è§£æé…ç½®é¡¹"""
        self.high_priority_keywords = set()
        if self.valves.high_priority_content:
            self.high_priority_keywords = {
                keyword.strip().lower()
                for keyword in self.valves.high_priority_content.split(",")
                if keyword.strip()
            }

    def _normalize_model_name(self, model_name: str) -> str:
        """æ ‡å‡†åŒ–æ¨¡å‹åï¼Œç”¨äºè¿è¡Œæ—¶èƒ½åŠ›ç¼“å­˜"""
        return (model_name or "").strip().lower()

    def _extract_error_signals_regex(self, text: str) -> Dict[str, Any]:
        """ä»é”™è¯¯æ–‡æœ¬ä¸­æå–æ¨¡å‹èƒ½åŠ›ä¿¡å·ï¼ˆæ­£åˆ™å…œåº•ï¼‰"""
        if not text:
            return {}

        lowered = text.lower()
        signals: Dict[str, Any] = {}

        token_patterns = [
            r"maximum context length is\s*(\d+)",
            r"model(?:'s)? maximum context length is\s*(\d+)",
            r"max(?:imum)?(?:\s+input)?\s+tokens?\s*(?:is|are|:)\s*(\d+)",
            r"æœ€å¤§(?:ä¸Šä¸‹æ–‡)?(?:é•¿åº¦|token(?:æ•°)?)\s*(?:ä¸º|æ˜¯|:)\s*(\d+)",
        ]
        for pattern in token_patterns:
            match = re.search(pattern, lowered, flags=re.IGNORECASE)
            if match:
                try:
                    parsed_limit = int(match.group(1))
                    if parsed_limit > 0:
                        signals["limit"] = parsed_limit
                        break
                except Exception:
                    continue

        multimodal_unsupported_patterns = [
            r"does not support (?:image|vision|multimodal)",
            r"image(?:_url)?(?: input)? is not supported",
            r"vision is not supported",
            r"only supports text",
            r"ä¸æ”¯æŒ(?:å›¾ç‰‡|å›¾åƒ|è§†è§‰|å¤šæ¨¡æ€)",
            r"ä»…æ”¯æŒæ–‡æœ¬",
        ]
        if any(
            re.search(pattern, lowered, flags=re.IGNORECASE)
            for pattern in multimodal_unsupported_patterns
        ):
            signals["multimodal"] = False
            signals["image_tokens"] = 0

        return signals

    def _extract_json_object(self, text: str) -> Optional[dict]:
        """ä»æ–‡æœ¬ä¸­æå–JSONå¯¹è±¡"""
        if not text:
            return None
        text = text.strip()
        try:
            parsed = json.loads(text)
            return parsed if isinstance(parsed, dict) else None
        except Exception:
            pass

        match = re.search(r"\{[\s\S]*\}", text)
        if not match:
            return None
        try:
            parsed = json.loads(match.group(0))
            return parsed if isinstance(parsed, dict) else None
        except Exception:
            return None

    async def _extract_error_signals_with_text_model(
        self, error_text: str
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æ–‡æœ¬æ¨¡å‹è§£æé”™è¯¯ï¼Œæå–æ¨¡å‹èƒ½åŠ›ä¿¡å·"""
        if not error_text:
            return {}

        client = self.get_api_client()
        if not client:
            return {}

        system_prompt = (
            "ä½ æ˜¯APIé”™è¯¯è§£æå™¨ã€‚è¯·ä»é”™è¯¯æ–‡æœ¬ä¸­è¯†åˆ«æ¨¡å‹èƒ½åŠ›ä¿¡å·ï¼Œå¹¶ä¸¥æ ¼è¾“å‡ºJSONã€‚"
            "è‹¥æ— æ³•åˆ¤æ–­æŸå­—æ®µï¼Œå¡« nullã€‚ä¸è¦è¾“å‡ºé¢å¤–æ–‡æœ¬ã€‚"
        )
        user_prompt = (
            "é”™è¯¯æ–‡æœ¬ï¼š\n"
            f"{error_text}\n\n"
            "è¯·è¾“å‡ºï¼š"
            '{"limit": <int|null>, "multimodal": <true|false|null>, "image_tokens": <int|null>}'
        )

        try:
            response = await client.chat.completions.create(
                model=self.valves.text_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=120,
                temperature=0,
                timeout=self.valves.request_timeout,
            )
        except Exception:
            return {}

        if not response or not response.choices:
            return {}

        content = (response.choices[0].message.content or "").strip()
        parsed = self._extract_json_object(content)
        if not parsed:
            return {}

        signals: Dict[str, Any] = {}

        limit_val = parsed.get("limit")
        if isinstance(limit_val, (int, float)) and int(limit_val) > 0:
            signals["limit"] = int(limit_val)
        elif isinstance(limit_val, str) and limit_val.strip().isdigit():
            signals["limit"] = int(limit_val.strip())

        multimodal_val = parsed.get("multimodal")
        if isinstance(multimodal_val, bool):
            signals["multimodal"] = multimodal_val

        image_tokens_val = parsed.get("image_tokens")
        if isinstance(image_tokens_val, (int, float)) and int(image_tokens_val) >= 0:
            signals["image_tokens"] = int(image_tokens_val)
        elif isinstance(image_tokens_val, str) and image_tokens_val.strip().isdigit():
            signals["image_tokens"] = int(image_tokens_val.strip())

        if signals.get("multimodal") is False and "image_tokens" not in signals:
            signals["image_tokens"] = 0

        return signals

    async def learn_model_capability_from_errors(
        self,
        model_name: str,
        error_text: str = "",
    ):
        """ä»è¯·æ±‚è¿”å›é”™è¯¯ä¸­å­¦ä¹ æ¨¡å‹èƒ½åŠ›ï¼Œè¦†ç›–é™æ€å­—å…¸"""
        model_key = self._normalize_model_name(model_name)
        if not model_key or not error_text:
            return

        regex_signals = self._extract_error_signals_regex(error_text)
        llm_signals = await self._extract_error_signals_with_text_model(error_text)

        merged_signals: Dict[str, Any] = {}
        merged_signals.update(regex_signals)
        merged_signals.update(llm_signals)

        if not merged_signals:
            return

        existing = self.model_runtime_overrides.get(model_key, {})
        existing.update(merged_signals)
        self.model_runtime_overrides[model_key] = existing
        self.debug_log(
            1,
            f"å·²ä»é”™è¯¯ä¿¡æ¯å­¦ä¹ æ¨¡å‹èƒ½åŠ›: {model_name} -> {existing}",
            "ğŸ§ ",
        )

    def reset_processing_state(self):
        """é‡ç½®å¤„ç†çŠ¶æ€"""
        self.current_processing_id = None
        self.message_order = None
        self.current_user_message = None
        self.current_model_info = None
        self.stats = ProcessingStats()
        if self.embedding_cache:
            self.embedding_cache.clear()

    def debug_log(self, level: int, message: str, emoji: str = "ğŸ”§"):
        """åˆ†çº§è°ƒè¯•æ—¥å¿—"""
        if self.valves.debug_level >= level:
            prefix = ["", "ğŸ›[DEBUG]", "ğŸ”[DETAIL]", "ğŸ“‹[VERBOSE]"][min(level, 3)]
            message = self.input_cleaner.clean_text_for_regex(message)
            print(f"{prefix} {emoji} {message}")

    # ========== Memoryç›¸å…³æ–¹æ³• ==========

    def memory_log(self, message: str, level: str = "info"):
        """è®°å¿†ç³»ç»Ÿæ—¥å¿—ï¼ˆä¼˜å…ˆå†™å…¥ loggingï¼ŒåŒæ—¶ print flush ä¾¿äºåœ¨ç»ˆç«¯çœ‹åˆ°ï¼‰"""
        if not getattr(self.valves, "enable_auto_memory", False):
            return

        prefix_map = {
            "debug": "ğŸ”",
            "info": "â„¹ï¸",
            "warning": "âš ï¸",
            "error": "âŒ",
        }
        prefix = prefix_map.get(level, "â„¹ï¸")
        try:
            message = self.input_cleaner.clean_text_for_regex(message)
        except Exception:
            pass

        # ç»ˆç«¯è¾“å‡ºï¼ˆç¡®ä¿ flushï¼‰
        try:
            print(f"{prefix} [AutoMemory] {message}", flush=True)
        except Exception:
            pass

        # logging è¾“å‡ºï¼ˆç»™ uvicorn / docker logsï¼‰
        try:
            logger = getattr(self, "logger", None)
            if logger:
                if level == "debug":
                    logger.debug(message)
                elif level == "warning":
                    logger.warning(message)
                elif level == "error":
                    logger.error(message)
                else:
                    logger.info(message)
        except Exception:
            pass

    def messages_to_string_for_memory(self, messages: list[dict]) -> str:
        """å°†æ¶ˆæ¯è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ä¾›è®°å¿†ç³»ç»Ÿä½¿ç”¨"""
        stringified_messages = []
        effective_count = self.valves.memory_messages_to_consider

        for i in range(1, effective_count + 1):
            if i > len(messages):
                break
            try:
                message = messages[-i]
                stringified_messages.append(
                    STRINGIFIED_MESSAGE_TEMPLATE.format(
                        index=i,
                        role=message.get("role", "user"),
                        content=self.extract_text_from_content(
                            message.get("content", "")
                        ),
                    )
                )
            except Exception as e:
                self.memory_log(f"æ¶ˆæ¯å­—ç¬¦ä¸²åŒ–å¤±è´¥ {i}: {e}", "warning")

        return "\n".join(stringified_messages)

    async def get_related_memories_for_auto_memory(
        self, messages: list[dict], user: UserModel
    ) -> list[Memory]:
        """è·å–ç›¸å…³è®°å¿†"""
        memory_query = self.build_memory_query_from_messages(messages)

        try:
            results = await query_memory(
                request=Request(scope={"type": "http", "app": webui_app}),
                form_data=QueryMemoryForm(
                    content=memory_query, k=self.valves.memory_related_memories_n
                ),
                user=user,
            )
        except HTTPException as e:
            if e.status_code == 404:
                # Open WebUI å¸¸è§è¡Œä¸ºï¼šå½“ç”¨æˆ·å°šæ— ä»»ä½•è®°å¿†æ—¶ï¼Œä¼šè¿”å› 404ï¼ˆä¾‹å¦‚ detail ä¸º "No memories found for user"ï¼‰ã€‚
                # è¿™ä¸ä»£è¡¨ Memory åŠŸèƒ½ä¸å¯ç”¨ï¼›ä»…è¡¨ç¤ºå½“å‰æ²¡æœ‰å¯æ£€ç´¢çš„è®°å¿†ã€‚
                self.memory_log(f"æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ï¼ˆ404ï¼‰: {e.detail}", "info")
                return []
            else:
                self.memory_log(f"è®°å¿†æŸ¥è¯¢å¤±è´¥ {e.status_code}: {e.detail}", "error")
                raise RuntimeError("è®°å¿†æŸ¥è¯¢å¤±è´¥") from e
        except Exception as e:
            self.memory_log(f"è®°å¿†æŸ¥è¯¢å¼‚å¸¸: {e}", "error")
            raise RuntimeError("è®°å¿†æŸ¥è¯¢å¤±è´¥") from e

        related_memories = searchresults_to_memories(results) if results else []
        self.memory_log(f"æ‰¾åˆ° {len(related_memories)} æ¡ç›¸å…³è®°å¿†", "info")

        if self.valves.memory_minimum_similarity is not None:
            filtered_memories = [
                mem
                for mem in related_memories
                if mem.similarity_score is not None
                and mem.similarity_score >= self.valves.memory_minimum_similarity
            ]
            filtered_count = len(related_memories) - len(filtered_memories)
            if filtered_count > 0:
                self.memory_log(f"è¿‡æ»¤æ‰ {filtered_count} æ¡ä½ç›¸ä¼¼åº¦è®°å¿†", "info")
            related_memories = filtered_memories

        return related_memories

    def build_memory_query_from_messages(self, messages: list[dict]) -> str:
        """ä»æ¶ˆæ¯æ„å»ºè®°å¿†æŸ¥è¯¢"""
        query_parts = []

        last_user_idx = None
        last_user_msg = None
        for idx in range(len(messages) - 1, -1, -1):
            if messages[idx].get("role") == "user":
                last_user_idx = idx
                last_user_msg = messages[idx].get("content", "")
                break

        if last_user_msg is None or last_user_idx is None:
            return ""

        user_text = self.extract_text_from_content(last_user_msg)
        user_word_count = len(user_text.split())
        include_context = user_word_count <= 8

        if last_user_idx + 1 < len(messages):
            assistant_msg = self.extract_text_from_content(
                messages[last_user_idx + 1].get("content", "")
            )
            if assistant_msg:
                query_parts.append(f"Assistant: {assistant_msg}")

        query_parts.append(f"User: {user_text}")

        if include_context and last_user_idx > 0:
            prev_msg = self.extract_text_from_content(
                messages[last_user_idx - 1].get("content", "")
            )
            if prev_msg and messages[last_user_idx - 1].get("role") == "assistant":
                query_parts.append(f"Assistant: {prev_msg}")

        query_parts.reverse()
        return "\n".join(query_parts)

    async def query_memory_llm_for_actions(
        self,
        conversation_str: str,
        stringified_memories: str,
        existing_ids: list[str],
        event_emitter,
    ):
        """è°ƒç”¨ LLM è·å–è®°å¿†æ“ä½œï¼ˆå…¼å®¹ openai-compatibleï¼šä¸ä¾èµ– .parseï¼‰"""
        client = self.get_api_client()
        if not client:
            self.memory_log(
                "APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼ˆè¯·æ£€æŸ¥ api_base/api_key é…ç½®ï¼‰", "error"
            )
            return None

        model_to_use = self.valves.memory_model or self.valves.model
        ids_hint = ", ".join(existing_ids) if existing_ids else "(none)"
        output_rules = (
            AUTO_MEMORY_OUTPUT_INSTRUCTIONS
            + "\nExisting IDs for update/delete:\n"
            + ids_hint
            + "\n"
        )

        messages = [
            {"role": "system", "content": UNIFIED_SYSTEM_PROMPT},
            {
                "role": "user",
                "content": (
                    'Conversation snippet (plain text, latest user message is the LAST line that starts with "User:"):\n'
                    f"{conversation_str}\n\n"
                    "Related Memories (may be empty):\n"
                    f"{stringified_memories}\n\n"
                    f"{output_rules}"
                ),
            },
        ]

        # ç»Ÿä¸€èµ° createï¼›æŸäº›ç¯å¢ƒ openai==1.x å¯èƒ½æ²¡æœ‰ chat.completions.parse
        try:
            response = await client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=0.3,
                timeout=self.valves.request_timeout,
            )
        except TypeError:
            # æŸäº›å®ç°ä¸æ”¯æŒ timeout å‚æ•°
            response = await client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=0.3,
            )
        except Exception as e:
            self.memory_log(f"LLMè°ƒç”¨å¤±è´¥: {str(e)[:200]}", "error")
            return None

        try:
            if not response.choices:
                self.memory_log("LLMè¿”å›ç©º choices", "error")
                return None

            text_response = (response.choices[0].message.content or "").strip()
            if not text_response:
                self.memory_log("LLMè¿”å›ç©ºå“åº”", "error")
                return None

            # å»æ‰å¯èƒ½çš„ code fence
            if text_response.startswith("```"):
                text_response = re.sub(r"^```[a-zA-Z0-9_+-]*\n", "", text_response)
                text_response = re.sub(r"\n```\s*$", "", text_response).strip()

            response_model = build_actions_request_model(existing_ids)

            # å°è¯•æå– JSON å¯¹è±¡ï¼ˆå…¼å®¹å‰åå¤¹æ‚è§£é‡Šæ–‡å­—çš„æƒ…å†µï¼‰
            json_str = text_response
            l = json_str.find("{")
            r = json_str.rfind("}")
            if l != -1 and r != -1 and r > l:
                json_str = json_str[l : r + 1]
            else:
                # å¸¸è§ï¼šæ¨¡å‹ç›´æ¥è¿”å›â€œno actions needed ...â€ç­‰é JSON æ–‡æœ¬
                if re.search(
                    r"\bno actions?\b|\bno action needed\b", json_str, re.IGNORECASE
                ):
                    self.memory_log(
                        "LLMè¿”å›éJSONçš„æ— æ“ä½œç»“æœï¼Œè§†ä¸º actions=[]", "info"
                    )
                    return response_model.model_validate(
                        {
                            "actions": [],
                            "reason": "LLM returned non-JSON no-action result",
                        }
                    )
                if not json_str.strip():
                    self.memory_log("LLMè¿”å›ç©ºç»“æœï¼Œè§†ä¸º actions=[]", "info")
                    return response_model.model_validate(
                        {
                            "actions": [],
                            "reason": "LLM returned non-JSON no-action result",
                        }
                    )

            try:
                return response_model.model_validate_json(json_str)
            except ValidationError:
                # æœ‰äº›æ¨¡å‹ä¼šè¿”å›å•å¼•å·/å°¾é€—å·ç­‰ï¼Œé™çº§ï¼šå…ˆ json.loads å† validate
                obj = json.loads(json_str)
                return response_model.model_validate(obj)

        except Exception as e:
            preview = (locals().get("text_response", "") or "")[:400]
            self.memory_log(f"è®°å¿†æ“ä½œè§£æå¤±è´¥: {e}; raw={preview!r}", "error")
            return None

    async def apply_memory_actions(
        self,
        action_plan: MemoryActionRequestStub,
        user: UserModel,
        emitter: Callable,
    ):
        """æ‰§è¡Œè®°å¿†æ“ä½œï¼ˆå³ä½¿ 0 actions ä¹Ÿä¼šæ‰“å°æ—¥å¿—ï¼Œä¾¿äºæ’éšœï¼‰"""
        if not action_plan:
            self.memory_log("action_plan ä¸ºç©ºï¼Œè·³è¿‡è®°å¿†æ“ä½œ", "warning")
            return

        actions = list(getattr(action_plan, "actions", None) or [])
        self.memory_log(f"æ”¶åˆ° {len(actions)} ä¸ªè®°å¿†æ“ä½œ", "info")

        reason = (getattr(action_plan, "reason", "") or "").strip()
        if not actions and reason:
            # è®© actions=0 çš„æƒ…å†µæ›´ç›´è§‚
            self.memory_log(f"æ— è®°å¿†æ›´æ–°åŸå› : {reason}", "info")

        if not actions:
            # 0 actions çš„æƒ…å†µå¾ˆå¸¸è§ï¼ˆLLM åˆ¤æ–­æ— éœ€å†™å…¥/æ›´æ–°/åˆ é™¤è®°å¿†ï¼‰

            self.memory_log("æ— è®°å¿†æ›´æ–°ï¼ˆactions=0ï¼‰", "info")
            return

        self.memory_log(f"å¼€å§‹æ‰§è¡Œ {len(actions)} ä¸ªè®°å¿†æ“ä½œ", "info")

        operations = {
            "delete": {
                "actions": [a for a in actions if a.action == "delete"],
                "handler": lambda a: delete_memory_by_id(memory_id=a.id, user=user),
                "status_verb": "deleted",
            },
            "update": {
                "actions": [a for a in actions if a.action == "update"],
                "handler": lambda a: update_memory_by_id(
                    memory_id=a.id,
                    request=Request(scope={"type": "http", "app": webui_app}),
                    form_data=MemoryUpdateModel(content=a.content),
                    user=user,
                ),
                "status_verb": "updated",
            },
            "add": {
                "actions": [a for a in actions if a.action == "add"],
                "handler": lambda a: add_memory(
                    request=Request(scope={"type": "http", "app": webui_app}),
                    form_data=AddMemoryForm(content=a.content),
                    user=user,
                ),
                "status_verb": "saved",
            },
        }

        counts = {}
        for op_name, op_config in operations.items():
            counts[op_name] = 0
            for action in op_config["actions"]:
                if op_name in ["add", "update"]:
                    content = getattr(action, "content", "")
                    if not content or not content.strip():
                        continue

                try:
                    await op_config["handler"](action)
                    counts[op_name] += 1
                    self.memory_log(
                        f"{op_config['status_verb']}: {getattr(action, 'id', 'new')}",
                        "info",
                    )
                except Exception as e:
                    # å…¼å®¹ä¸åŒ Open WebUI ç‰ˆæœ¬ï¼šéƒ¨åˆ†ç‰ˆæœ¬çš„ router ä¼šå› ä¸ºâ€œMemory å¼€å…³/æƒé™/è·¯ç”±å·®å¼‚â€ç­‰è¿”å› 404/403
                    # è¿™é‡Œåš DB å±‚ fallbackï¼ˆä¸ç¤¾åŒº Memory å·¥å…·åŒä¸€è·¯å¾„ï¼šopen_webui.models.memories.Memoriesï¼‰
                    self.memory_log(
                        f"æ“ä½œå¤±è´¥ ({op_name})ï¼Œå‡†å¤‡å°è¯• fallback: {str(e)[:120]}",
                        "warning",
                    )

                    try:
                        from open_webui.models.memories import Memories  # type: ignore

                        if op_name == "add":
                            content = getattr(action, "content", "")
                            fb = Memories.insert_new_memory(user.id, content)
                            if fb:
                                counts[op_name] += 1
                                self.memory_log(
                                    f"fallback saved: {getattr(fb, 'id', 'new')}",
                                    "info",
                                )
                                continue
                        elif op_name == "update":
                            content = getattr(action, "content", "")
                            fb = Memories.update_memory_by_id(action.id, content)
                            if fb:
                                counts[op_name] += 1
                                self.memory_log(
                                    f"fallback updated: {action.id}", "info"
                                )
                                continue
                        elif op_name == "delete":
                            fb = Memories.delete_memory_by_id(action.id)
                            if fb:
                                counts[op_name] += 1
                                self.memory_log(
                                    f"fallback deleted: {action.id}", "info"
                                )
                                continue
                    except Exception as fb_e:
                        self.memory_log(
                            f"fallback å¤±è´¥ ({op_name}): {str(fb_e)[:120]}", "error"
                        )

                    self.memory_log(
                        f"æ“ä½œå¤±è´¥ ({op_name}) ä¸” fallback å¤±è´¥: {str(e)[:120]}",
                        "error",
                    )

        status_parts = []
        for op_name, op_config in operations.items():
            count = counts[op_name]
            if count > 0:
                memory_word = "memory" if count == 1 else "memories"
                status_parts.append(f"{op_config['status_verb']} {count} {memory_word}")

        status_message = ", ".join(status_parts)
        if status_message:
            self.memory_log(f"è®°å¿†æ“ä½œç»“æœ: {status_message}", "info")

        # é¢å¤–ï¼šæ‰“å° DB ä¸­çš„è®°å¿†æ¡æ•°ï¼Œæ–¹ä¾¿åˆ¤æ–­â€œåˆ°åº•æœ‰æ²¡æœ‰å†™è¿›å»â€
        try:
            from open_webui.models.memories import Memories  # type: ignore

            _all = Memories.get_memories_by_user_id(user.id) or []
            self.memory_log(f"å½“å‰æ•°æ®åº“è®°å¿†æ¡æ•°: {len(_all)}", "info")
            if _all:
                _latest = sorted(_all, key=lambda m: getattr(m, "created_at", 0))[-1]
                _preview = (getattr(_latest, "content", "") or "")[:80].replace(
                    "\n", " "
                )
                self.memory_log(
                    f"æœ€æ–°è®°å¿†é¢„è§ˆ: {getattr(_latest, 'id', '')} :: {_preview!r}",
                    "debug",
                )
        except Exception as e:
            self.memory_log(
                f"è¯»å–æ•°æ®åº“è®°å¿†å¤±è´¥(ä»…ç”¨äºæ—¥å¿—): {str(e)[:120]}", "warning"
            )

        self.memory_log(status_message or "æ— è®°å¿†æ›´æ–°", "info")

    async def auto_memory_process(
        self,
        messages: list[dict],
        user: UserModel,
        emitter: Callable,
    ):
        """è‡ªåŠ¨è®°å¿†å¤„ç†ä¸»æµç¨‹"""
        if len(messages) < 2:
            self.memory_log("æ¶ˆæ¯æ•°ä¸è¶³ï¼Œè·³è¿‡è®°å¿†å¤„ç†", "debug")
            return

        self.memory_log(f"å¼€å§‹è®°å¿†å¤„ç† - ç”¨æˆ·: {user.id}", "info")

        try:
            # 1) å¯é€‰ï¼šå¼ºåˆ¶å†™å…¥è®°å¿†ï¼ˆç”¨äºè°ƒè¯•/æ˜¾å¼æŒ‡ä»¤ï¼‰ï¼Œå‘½ä¸­åˆ™è·³è¿‡LLMåˆ¤æ–­
            prefixes_raw = (
                getattr(self.valves, "memory_force_add_prefixes", "") or ""
            ).strip()
            if prefixes_raw:
                prefixes = [p.strip() for p in prefixes_raw.split(";") if p.strip()]
                latest_user = next(
                    (m for m in reversed(messages) if m.get("role") == "user"), None
                )
                latest_content = (latest_user or {}).get("content", "")
                # å°† content ç»Ÿä¸€è½¬ä¸ºçº¯æ–‡æœ¬
                if isinstance(latest_content, list):
                    text_parts = []
                    for part in latest_content:
                        if isinstance(part, dict):
                            if part.get("type") == "text" and part.get("text"):
                                text_parts.append(str(part.get("text")))
                        elif isinstance(part, str):
                            text_parts.append(part)
                    latest_text = "\n".join([t for t in text_parts if t]).strip()
                else:
                    latest_text = str(latest_content or "").strip()

                for pfx in prefixes:
                    if latest_text.lower().startswith(pfx.lower()):
                        forced_text = latest_text[len(pfx) :].strip()
                        if forced_text:
                            self.memory_log(
                                f"æ£€æµ‹åˆ°å¼ºåˆ¶è®°å¿†å‰ç¼€ï¼Œç›´æ¥å†™å…¥: {forced_text[:120]}",
                                "info",
                            )
                            forced_plan = MemoryActionRequestStub(
                                actions=[
                                    MemoryAddAction(action="add", content=forced_text)
                                ],
                                reason=f"forced_prefix:{pfx}",
                            )
                            await self.apply_memory_actions(forced_plan, user, emitter)
                            self.memory_log("è®°å¿†å¤„ç†å®Œæˆï¼ˆå¼ºåˆ¶å†™å…¥ï¼‰", "info")
                            return

            # 2) æ­£å¸¸ï¼šæ£€ç´¢ç›¸å…³è®°å¿† + è®©LLMå†³å®šå†™å…¥/æ›´æ–°/åˆ é™¤
            related_memories = await self.get_related_memories_for_auto_memory(
                messages, user
            )
            stringified_memories = json.dumps(
                [memory.model_dump(mode="json") for memory in related_memories]
            )
            conversation_str = self.messages_to_string_for_memory(messages)
            existing_ids = [m.mem_id for m in related_memories]

            action_plan = await self.query_memory_llm_for_actions(
                conversation_str, stringified_memories, existing_ids, emitter
            )

            if not action_plan:
                self.memory_log("LLMæœªè¿”å›æœ‰æ•ˆæ“ä½œ", "warning")
                return

            await self.apply_memory_actions(action_plan, user, emitter)
            self.memory_log("è®°å¿†å¤„ç†å®Œæˆ", "info")

        except Exception as e:
            self.memory_log(f"è®°å¿†å¤„ç†å¼‚å¸¸: {str(e)[:200]}", "error")
            import traceback

            if self.valves.debug_level >= 2:
                traceback.print_exc()

    def extract_memory_context(self, content: str) -> Optional[tuple[str, list[dict]]]:
        """ä»ç³»ç»Ÿæ¶ˆæ¯ä¸­æå–è®°å¿†ä¸Šä¸‹æ–‡"""
        pattern = r"<memory_user_context>\s*(\[[\s\S]*?\])\s*</memory_user_context>"
        match = re.search(pattern, content)
        if not match:
            return None

        try:
            memories_json = match.group(1)
            memories_list = json.loads(memories_json)
            self.memory_log(f"æå–åˆ° {len(memories_list)} æ¡è®°å¿†", "debug")
            return (match.group(0), memories_list)
        except json.JSONDecodeError as e:
            self.memory_log(f"è®°å¿†ä¸Šä¸‹æ–‡JSONè§£æå¤±è´¥: {e}", "error")
            return None

    def format_memory_context(self, memories: list[dict]) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡"""
        memories = [
            {k: v for k, v in mem.items() if k != "similarity_score"}
            for mem in memories
        ]
        memories_json = json.dumps(memories, indent=2, ensure_ascii=False)
        return f"<long_term_memory>\n{memories_json}\n</long_term_memory>"

    def process_memory_context_in_messages(self, messages: list[dict]) -> list[dict]:
        """å¤„ç†æ¶ˆæ¯ä¸­çš„è®°å¿†ä¸Šä¸‹æ–‡"""
        if not self.valves.override_memory_context:
            return messages

        found_any = False
        for i, message in enumerate(messages):
            if message.get("role") != "system":
                continue

            content = message.get("content", "")
            if not content:
                continue

            extraction_result = self.extract_memory_context(content)
            if extraction_result:
                found_any = True
                full_match, memories_list = extraction_result
                new_context = self.format_memory_context(memories_list)
                messages[i]["content"] = content.replace(full_match, new_context)
                self.memory_log(
                    f"è¦†ç›–ç³»ç»Ÿæ¶ˆæ¯{i}çš„è®°å¿†ä¸Šä¸‹æ–‡: {len(memories_list)}æ¡è®°å¿†", "info"
                )

        if not found_any and self.valves.override_memory_context:
            self.memory_log("æœªæ‰¾åˆ°è®°å¿†ä¸Šä¸‹æ–‡æ ‡ç­¾", "warning")

        return messages

    # ========== å·¥å…·æ–¹æ³• ==========

    def is_model_excluded(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«æ’é™¤"""
        if not self.valves.excluded_models or not model_name:
            return False
        excluded_list = [
            model.strip().lower()
            for model in self.valves.excluded_models.split(",")
            if model.strip()
        ]
        if not excluded_list:
            return False
        model_lower = model_name.lower()
        for excluded_model in excluded_list:
            if excluded_model in model_lower:
                self.debug_log(1, f"æ¨¡å‹ {model_name} åœ¨æ’é™¤åˆ—è¡¨ä¸­", "ğŸš«")
                return True
        return False

    def analyze_model(self, model_name: str) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹ä¿¡æ¯"""
        model_info = self.model_matcher.match_model(model_name)

        model_key = self._normalize_model_name(model_name)
        runtime_override = self.model_runtime_overrides.get(model_key)
        if runtime_override:
            model_info.update(runtime_override)
            model_info["match_type"] = "runtime"

        self.token_calculator.set_model_info(model_info)

        multimodal_status = "å¤šæ¨¡æ€" if model_info["multimodal"] else "æ–‡æœ¬"
        family_name = model_info["family"].upper()
        tokens_display = f"{model_info['limit']:,}tokens"
        match_type = model_info.get("match_type")
        if match_type == "exact":
            match_type_display = "ç²¾ç¡®"
        elif match_type == "fuzzy":
            match_type_display = "æ¨¡ç³Š"
        elif match_type == "runtime":
            match_type_display = "é”™è¯¯å­¦ä¹ "
        else:
            match_type_display = "é»˜è®¤"

        print(f"ğŸ¯ æ¨¡å‹è¯†åˆ«: {model_name}")
        print(f"   â”œâ”€ ç³»åˆ—: {family_name}")
        print(f"   â”œâ”€ ç±»å‹: {multimodal_status}")
        print(f"   â”œâ”€ é™åˆ¶: {tokens_display}")
        print(f"   â””â”€ åŒ¹é…: {match_type_display}åŒ¹é…")
        if model_info.get("hint"):
            print(f"   âš ï¸ æç¤º: {model_info['hint']}")

        if model_info.get("special") == "thinking":
            print(f"   ğŸ’­ ç‰¹æ®Š: Thinkingæ¨¡å‹")

        if model_info.get("family") == "gpt" and "gpt-5" in model_name.lower():
            print(f"   ğŸ†• æ–°æ¨¡å‹: GPT-5ç³»åˆ— (200k tokens + å¤šæ¨¡æ€)")

        return model_info

    def count_tokens(self, text: str) -> int:
        """ç®€åŒ–çš„tokenè®¡ç®—"""
        if not text:
            return 0
        return self.token_calculator.count_tokens(text)

    def count_message_tokens(self, message: dict) -> int:
        """è®¡ç®—å•æ¡æ¶ˆæ¯çš„tokenæ•°é‡"""
        if not message:
            return 0
        content = message.get("content", "")
        role = message.get("role", "")
        total_tokens = 0

        if isinstance(content, list):
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    total_tokens += self.count_tokens(text)
                elif item.get("type") == "image_url":
                    total_tokens += self.token_calculator.calculate_image_tokens("")
        else:
            total_tokens = self.count_tokens(content)

        total_tokens += self.count_tokens(role) + 20
        return total_tokens

    def count_messages_tokens(self, messages: List[dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€»tokenæ•°é‡"""
        if not messages:
            return 0
        total_tokens = sum(self.count_message_tokens(msg) for msg in messages)
        self.debug_log(
            2,
            f"æ¶ˆæ¯åˆ—è¡¨tokenè®¡ç®—: {len(messages)}æ¡æ¶ˆæ¯ -> {total_tokens:,}tokens",
            "ğŸ“Š",
        )
        return total_tokens

    def strip_internal_fields(self, messages: List[dict]) -> List[dict]:
        """ç§»é™¤æ¶ˆæ¯ä¸­çš„å†…éƒ¨å­—æ®µ"""
        if not messages:
            return []

        cleaned_messages: List[dict] = []
        for msg in messages:
            if not isinstance(msg, dict):
                cleaned_messages.append(msg)
                continue

            new_msg: dict = {k: v for k, v in msg.items() if not str(k).startswith("_")}
            content = new_msg.get("content")

            if isinstance(content, list):
                new_content = []
                for item in content:
                    if isinstance(item, dict):
                        new_item = {
                            k: v for k, v in item.items() if not str(k).startswith("_")
                        }
                        new_content.append(new_item)
                    else:
                        new_content.append(item)
                new_msg["content"] = new_content

            cleaned_messages.append(new_msg)

        return cleaned_messages

    def get_model_token_limit(self, model_name: str) -> int:
        """è·å–æ¨¡å‹çš„tokené™åˆ¶"""
        model_info = self.analyze_model(model_name)
        limit = model_info.get("limit", self.valves.default_token_limit)
        safe_limit = int(limit * self.valves.token_safety_ratio)
        self.debug_log(
            2, f"æ¨¡å‹tokené™åˆ¶: {model_name} -> {limit} -> {safe_limit}", "âš–ï¸"
        )
        return safe_limit

    def is_multimodal_model(self, model_name: str) -> bool:
        """åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å…¥"""
        model_info = self.analyze_model(model_name)
        return model_info.get("multimodal", False)

    def find_current_user_message(self, messages: List[dict]) -> Optional[dict]:
        """æŸ¥æ‰¾å½“å‰ç”¨æˆ·æ¶ˆæ¯"""
        if not messages:
            return None
        for msg in reversed(messages):
            if msg.get("role") == "user":
                self.debug_log(
                    2,
                    f"æ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯: {len(self.extract_text_from_content(msg.get('content', '')))}å­—ç¬¦",
                    "ğŸ’¬",
                )
                return msg
        return None

    def separate_current_and_history_messages(
        self, messages: List[dict]
    ) -> Tuple[Optional[dict], List[dict]]:
        """åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯"""
        if not messages:
            return None, []

        current_user_message = None
        current_user_index = -1

        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("role") == "user":
                current_user_message = msg
                current_user_index = i
                break

        if not current_user_message:
            self.debug_log(1, "æœªæ‰¾åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œæ‰€æœ‰æ¶ˆæ¯ä½œä¸ºå†å²æ¶ˆæ¯å¤„ç†", "âš ï¸")
            return None, messages

        history_messages = messages[:current_user_index]
        self.stats.history_message_separation_count += 1
        self.debug_log(
            1,
            f"æ¶ˆæ¯åˆ†ç¦»å®Œæˆ: å½“å‰ç”¨æˆ·æ¶ˆæ¯1æ¡({self.count_message_tokens(current_user_message)}tokens), å†å²æ¶ˆæ¯{len(history_messages)}æ¡({self.count_messages_tokens(history_messages):,}tokens)",
            "ğŸ“‹",
        )
        return current_user_message, history_messages

    def calculate_target_tokens(self, model_name: str, current_user_tokens: int) -> int:
        """è®¡ç®—ç›®æ ‡tokenæ•°"""
        model_token_limit = self.get_model_token_limit(model_name)
        response_buffer = min(
            self.valves.response_buffer_max,
            max(
                self.valves.response_buffer_min,
                int(model_token_limit * self.valves.response_buffer_ratio),
            ),
        )
        target_tokens = model_token_limit - current_user_tokens - response_buffer
        min_target = max(10000, model_token_limit * 0.3)
        target_tokens = max(target_tokens, min_target)
        self.debug_log(
            1,
            f"ç›®æ ‡tokenè®¡ç®—: {model_token_limit} - {current_user_tokens} - {response_buffer} = {target_tokens}",
            "ğŸ¯",
        )
        return int(target_tokens)

    def _needs_processing(
        self, messages: List[dict], model_name: str, target_tokens: int
    ):
        """åˆ¤å®šæ˜¯å¦éœ€è¦è¿›è¡Œå¤„ç†"""
        current_tokens = self.count_messages_tokens(messages)
        has_images = self.has_images_in_messages(messages)
        model_is_multimodal = self.is_multimodal_model(model_name)
        token_overflow = current_tokens > target_tokens
        multimodal_incompatible = has_images and (not model_is_multimodal)
        return (
            (token_overflow or multimodal_incompatible),
            token_overflow,
            multimodal_incompatible,
        )

    def should_force_maximize_content(
        self, messages: List[dict], target_tokens: int
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼ºåˆ¶è¿›è¡Œå†…å®¹æœ€å¤§åŒ–å¤„ç†"""
        current_tokens = self.count_messages_tokens(messages)
        return current_tokens > target_tokens

    # ========== å¤šæ¨¡æ€å¤„ç† ==========

    def has_images_in_content(self, content) -> bool:
        """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        if isinstance(content, list):
            return any(item.get("type") == "image_url" for item in content)
        return False

    def has_images_in_messages(self, messages: List[dict]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯åˆ—è¡¨ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡"""
        return any(self.has_images_in_content(msg.get("content")) for msg in messages)

    def extract_text_from_content(self, content) -> str:
        """ä»å†…å®¹ä¸­æå–æ–‡æœ¬"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if item.get("type") == "text":
                    text = item.get("text", "")
                    text_parts.append(text)
            return " ".join(text_parts)
        else:
            return str(content) if content else ""

    def extract_images_from_content(self, content) -> List[dict]:
        """ä»å†…å®¹ä¸­æå–å›¾ç‰‡ä¿¡æ¯"""
        if isinstance(content, list):
            images = []
            for item in content:
                if item.get("type") == "image_url":
                    images.append(item)
            return images
        return []

    def is_high_priority_content(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§å†…å®¹"""
        if not text or not self.high_priority_keywords:
            return False
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.high_priority_keywords)

    # ========== APIå®¢æˆ·ç«¯ç®¡ç† ==========

    def get_api_client(self, client_type: str = "default"):
        """è·å–APIå®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE:
            return None
        if self.valves.api_key:
            return AsyncOpenAI(
                base_url=self.valves.api_base,
                api_key=self.valves.api_key,
                timeout=self.valves.request_timeout,
            )
        return None

    # ========== å®‰å…¨APIè°ƒç”¨ ==========

    async def safe_api_call(self, call_func, call_name: str, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
        for attempt in range(self.valves.api_error_retry_times + 1):
            try:
                result = await call_func(*args, **kwargs)
                return result
            except Exception as e:
                error_msg = str(e)
                self.stats.api_failures += 1

                # å¤±è´¥å­¦ä¹ ï¼šä»æŠ¥é”™ä¸­å­¦ä¹ æ¨¡å‹èƒ½åŠ›ï¼ˆtokenä¸Šé™/å¤šæ¨¡æ€æ”¯æŒï¼‰
                try:
                    fallback_model = getattr(self, "_current_model_name", "")
                    await self.learn_model_capability_from_errors(
                        fallback_model,
                        error_text=error_msg,
                    )
                except Exception:
                    pass

                if attempt < self.valves.api_error_retry_times:
                    self.debug_log(
                        1,
                        f"{call_name} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œ{self.valves.api_error_retry_delay}ç§’åé‡è¯•",
                        "ğŸ”„",
                    )
                    await asyncio.sleep(self.valves.api_error_retry_delay)
                else:
                    self.debug_log(1, f"{call_name} æœ€ç»ˆå¤±è´¥: {error_msg[:100]}", "âŒ")
                    return None
        return None

    # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ ==========

    async def detect_context_max_need_impl(self, query_text: str, event_emitter):
        """å®é™…çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        cleaned_query = self.input_cleaner.clean_text_for_regex(query_text)
        prompt = f"{self.valves.context_max_detection_prompt}\n\n{cleaned_query}"

        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,
            temperature=0.1,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            result = response.choices[0].message.content.strip()
            result = self.input_cleaner.clean_text_for_regex(result)
            need_context_max = "éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–" in result
            self.debug_log(
                2, f"AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹ç»“æœ: {result} -> {need_context_max}", "ğŸ§ "
            )
            return need_context_max
        return None

    async def detect_context_max_need(self, query_text: str, event_emitter) -> bool:
        """ä½¿ç”¨AIæ£€æµ‹æ˜¯å¦éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–"""
        if not self.valves.enable_ai_context_max_detection:
            return self.is_context_max_need_simple(query_text)

        self.debug_log(1, f"AIæ£€æµ‹ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚: {query_text[:50]}...", "ğŸ§ ")
        need_context_max = await self.safe_api_call(
            self.detect_context_max_need_impl,
            "ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹",
            query_text,
            event_emitter,
        )

        if need_context_max is not None:
            self.stats.context_maximization_detections += 1
            self.debug_log(
                1,
                f"AIä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹å®Œæˆ: {'éœ€è¦' if need_context_max else 'ä¸éœ€è¦'}",
                "ğŸ§ ",
            )
            return need_context_max
        else:
            self.debug_log(1, f"AIæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•", "âš ï¸")
            return self.is_context_max_need_simple(query_text)

    def is_context_max_need_simple(self, query_text: str) -> bool:
        """ç®€å•çš„ä¸Šä¸‹æ–‡æœ€å¤§åŒ–éœ€æ±‚åˆ¤æ–­"""
        if not query_text:
            return True

        query_text = self.input_cleaner.clean_text_for_regex(query_text)
        context_max_patterns = [
            r".*èŠ.*ä»€ä¹ˆ.*",
            r".*è¯´.*ä»€ä¹ˆ.*",
            r".*è®¨è®º.*ä»€ä¹ˆ.*",
            r".*è°ˆ.*ä»€ä¹ˆ.*",
            r".*å†…å®¹.*",
            r".*è¯é¢˜.*",
            r".*å†å².*",
            r".*è®°å½•.*",
            r".*ä¹‹å‰.*",
            r"what.*discuss.*",
            r"what.*talk.*",
            r"what.*chat.*",
            r".*conversation.*",
            r".*history.*",
        ]

        query_lower = query_text.lower()
        for pattern in context_max_patterns:
            if self.input_cleaner.safe_regex_match(pattern, query_lower):
                return True

        return len(query_text.split()) <= 3

    # ========== å…³é”®å­—ç”Ÿæˆ ==========

    async def generate_keywords_impl(self, query_text: str, event_emitter):
        """å®é™…çš„å…³é”®å­—ç”Ÿæˆå®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        cleaned_query = self.input_cleaner.clean_text_for_regex(query_text)
        prompt = f"{self.valves.keyword_generation_prompt}\n\n{cleaned_query}"

        response = await client.chat.completions.create(
            model=self.valves.text_model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.3,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            keywords_text = response.choices[0].message.content.strip()
            keywords_text = self.input_cleaner.clean_text_for_regex(keywords_text)
            keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            keywords = [kw for kw in keywords if len(kw) >= 2]
            self.debug_log(2, f"ç”Ÿæˆå…³é”®å­—: {keywords[:5]}...", "ğŸ”‘")
            return keywords
        return None

    async def generate_search_keywords(
        self, query_text: str, event_emitter
    ) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®å­—"""
        if not self.valves.enable_keyword_generation:
            return [query_text]

        need_context_max = await self.detect_context_max_need(query_text, event_emitter)

        if not need_context_max and not self.valves.keyword_generation_for_context_max:
            self.debug_log(2, f"å…·ä½“æŸ¥è¯¢ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {query_text[:50]}...", "ğŸ”‘")
            return [query_text]

        self.debug_log(1, f"ç”Ÿæˆæœç´¢å…³é”®å­—: {query_text[:50]}...", "ğŸ”‘")
        keywords = await self.safe_api_call(
            self.generate_keywords_impl,
            "å…³é”®å­—ç”Ÿæˆ",
            query_text,
            event_emitter,
        )

        if keywords:
            final_keywords = [query_text] + keywords
            final_keywords = list(dict.fromkeys(final_keywords))
            self.stats.keyword_generations += 1
            self.debug_log(1, f"å…³é”®å­—ç”Ÿæˆå®Œæˆ: {len(final_keywords)}ä¸ª", "ğŸ”‘")
            return final_keywords
        else:
            self.debug_log(1, f"å…³é”®å­—ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢", "âš ï¸")
            return [query_text]

    # ========== å‘é‡å¤„ç† ==========

    async def get_text_embedding_impl(self, text: str, event_emitter):
        """å®é™…çš„æ–‡æœ¬å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        cleaned_text = self.input_cleaner.clean_text_for_regex(text)
        self.stats.embedding_requests += 1

        response = await client.embeddings.create(
            model=self.valves.text_vector_model,
            input=[cleaned_text[:8000]],
            encoding_format="float",
        )

        if (
            response
            and response.data
            and len(response.data) > 0
            and response.data[0].embedding
        ):
            return response.data[0].embedding
        return None

    async def get_text_embedding(
        self, text: str, event_emitter
    ) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡ - å¸¦ç¼“å­˜"""
        if not text:
            return None

        content_key = hashlib.md5(text.encode()).hexdigest()[:16]

        if self.embedding_cache:
            cached_embedding = self.embedding_cache.get(content_key)
            if cached_embedding:
                self.stats.cache_hits += 1
                self.debug_log(3, f"æ–‡æœ¬å‘é‡ç¼“å­˜å‘½ä¸­: {len(cached_embedding)}ç»´", "ğŸ’¾")
                return cached_embedding

        self.stats.cache_misses += 1
        embedding = await self.safe_api_call(
            self.get_text_embedding_impl,
            "æ–‡æœ¬å‘é‡",
            text,
            event_emitter,
        )

        if embedding:
            if self.embedding_cache:
                self.embedding_cache.set(content_key, embedding)
            self.debug_log(3, f"æ–‡æœ¬å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ“")

        return embedding

    async def get_multimodal_embedding_impl(self, content, event_emitter):
        """å®é™…çš„å¤šæ¨¡æ€å‘é‡è·å–å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        if isinstance(content, list):
            cleaned_content = []
            for item in content:
                if item.get("type") == "text":
                    cleaned_item = item.copy()
                    text = item.get("text", "")
                    cleaned_text = self.input_cleaner.clean_text_for_regex(text)
                    cleaned_item["text"] = cleaned_text
                    cleaned_content.append(cleaned_item)
                elif item.get("type") == "image_url":
                    image_url = item.get("image_url", {}).get("url", "")
                    is_valid, cleaned_url = (
                        self.input_cleaner.validate_and_clean_image_url(image_url)
                    )
                    if is_valid:
                        cleaned_item = copy.deepcopy(item)
                        cleaned_item["image_url"]["url"] = cleaned_url
                        cleaned_content.append(cleaned_item)
                else:
                    cleaned_content.append(item)
            input_data = cleaned_content
        else:
            text = str(content)
            cleaned_text = self.input_cleaner.clean_text_for_regex(text)
            input_data = [{"type": "text", "text": cleaned_text[:8000]}]

        self.stats.embedding_requests += 1

        try:
            response = await client.embeddings.create(
                model=self.valves.multimodal_vector_model,
                input=input_data,
                encoding_format="float",
            )

            if hasattr(response, "data") and hasattr(response.data, "embedding"):
                return response.data.embedding
            elif (
                hasattr(response, "data")
                and isinstance(response.data, list)
                and len(response.data) > 0
            ):
                return response.data[0].embedding
            else:
                self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡å“åº”æ ¼å¼å¼‚å¸¸", "âš ï¸")
                return None
        except Exception as e:
            self.debug_log(1, f"å¤šæ¨¡æ€å‘é‡è°ƒç”¨å¤±è´¥: {str(e)[:100]}", "âŒ")
            raise

    async def get_multimodal_embedding(
        self, content, event_emitter
    ) -> Optional[List[float]]:
        """è·å–å¤šæ¨¡æ€å‘é‡"""
        if not content:
            return None

        has_multimodal_content = False
        if isinstance(content, list):
            has_multimodal_content = any(
                item.get("type") in ["image_url", "video_url"] for item in content
            )

        if not has_multimodal_content:
            self.debug_log(3, "å†…å®¹ä¸åŒ…å«å¤šæ¨¡æ€å…ƒç´ ï¼Œä¸ä½¿ç”¨å¤šæ¨¡æ€å‘é‡", "ğŸ“")
            return None

        embedding = await self.safe_api_call(
            self.get_multimodal_embedding_impl,
            "å¤šæ¨¡æ€å‘é‡",
            content,
            event_emitter,
        )

        if embedding:
            self.debug_log(3, f"å¤šæ¨¡æ€å‘é‡è·å–æˆåŠŸ: {len(embedding)}ç»´", "ğŸ–¼ï¸")

        return embedding

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)

    # ========== ç›¸å…³åº¦è®¡ç®— ==========

    async def compute_relevance_scores(
        self, query_msg: dict, history_msgs: List[dict], progress: ProgressTracker
    ) -> List[dict]:
        """è®¡ç®—æ‰€æœ‰å†å²æ¶ˆæ¯çš„ç›¸å…³åº¦åˆ†æ•°"""
        if not history_msgs:
            return []

        self.debug_log(
            1, f"å¼€å§‹è®¡ç®—ç›¸å…³åº¦åˆ†æ•°: æŸ¥è¯¢1æ¡ï¼Œå†å²{len(history_msgs)}æ¡", "ğŸ¯"
        )

        query_content = query_msg.get("content", "")
        query_text = self.extract_text_from_content(query_content)

        if len(history_msgs) > 40:
            lightweight_scored = self._compute_lightweight_scores(
                query_text, history_msgs
            )
            top_k = min(self.valves.vector_top_k, len(lightweight_scored))
            lightweight_scored.sort(key=lambda x: x["score"], reverse=True)
            selected_msgs = lightweight_scored[:top_k]
            self.debug_log(
                1,
                f"ä¸¤é˜¶æ®µå¬å›: {len(history_msgs)} -> {len(selected_msgs)}æ¡è¿›å…¥å‘é‡åŒ–é˜¶æ®µ",
                "âš¡",
            )

            if len(selected_msgs) > 80:
                scored = selected_msgs
            else:
                if self.has_images_in_content(query_content):
                    query_vector = await self.get_multimodal_embedding(
                        query_content, progress.event_emitter
                    )
                    if not query_vector:
                        query_vector = await self.get_text_embedding(
                            query_text, progress.event_emitter
                        )
                else:
                    query_vector = await self.get_text_embedding(
                        query_text, progress.event_emitter
                    )

                scored = await self._compute_vector_scores_concurrent(
                    query_vector, selected_msgs, progress
                )
        else:
            if self.has_images_in_content(query_content):
                query_vector = await self.get_multimodal_embedding(
                    query_content, progress.event_emitter
                )
                if not query_vector:
                    query_vector = await self.get_text_embedding(
                        query_text, progress.event_emitter
                    )
            else:
                query_vector = await self.get_text_embedding(
                    query_text, progress.event_emitter
                )

            msg_items = []
            for idx, msg in enumerate(history_msgs):
                msg_items.append(
                    {"msg": msg, "idx": idx, "tokens": self.count_message_tokens(msg)}
                )

            scored = await self._compute_vector_scores_concurrent(
                query_vector, msg_items, progress
            )

        self.debug_log(1, f"ç›¸å…³åº¦è®¡ç®—å®Œæˆ: {len(scored)}æ¡æ¶ˆæ¯å…¨éƒ¨è¯„åˆ†", "ğŸ¯")

        if self.valves.debug_level >= 2:
            top5 = sorted(scored, key=lambda x: x["score"], reverse=True)[:5]
            for i, item in enumerate(top5):
                self.debug_log(
                    2,
                    f"Top{i+1}: score={item['score']:.3f}, {item['tokens']}tokens",
                    "ğŸ“Š",
                )

        return scored

    def _compute_lightweight_scores(
        self, query_text: str, history_msgs: List[dict]
    ) -> List[dict]:
        """è½»é‡çº§è¯„åˆ†"""
        scored = []
        query_lower = query_text.lower()
        query_words = set(query_lower.split())

        for idx, msg in enumerate(history_msgs):
            msg_content = msg.get("content", "")
            msg_text = self.extract_text_from_content(msg_content)
            msg_lower = msg_text.lower()
            msg_words = set(msg_lower.split())

            common_words = query_words & msg_words
            text_sim = (
                len(common_words) / max(1, len(query_words)) if query_words else 0
            )

            recency = idx / max(1, len(history_msgs) - 1)
            role = msg.get("role", "")
            role_weight = (
                1.0 if role == "user" else (0.8 if role == "assistant" else 0.6)
            )
            kw_bonus = 1.0 if self.is_high_priority_content(msg_text) else 0.0

            score = 0.6 * text_sim + 0.2 * recency + 0.1 * role_weight + 0.1 * kw_bonus

            scored.append(
                {
                    "msg": msg,
                    "score": score,
                    "tokens": self.count_message_tokens(msg),
                    "idx": idx,
                    "sim": text_sim,
                    "recency": recency,
                    "role_weight": role_weight,
                    "kw_bonus": kw_bonus,
                }
            )

        return scored

    async def _compute_vector_scores_concurrent(
        self,
        query_vector: List[float],
        msg_items: List[dict],
        progress: ProgressTracker,
    ) -> List[dict]:
        """å¹¶å‘è®¡ç®—å‘é‡åˆ†æ•°"""
        semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)

        async def get_msg_embedding(item):
            async with semaphore:
                msg = item["msg"]
                msg_content = msg.get("content", "")
                msg_text = self.extract_text_from_content(msg_content)

                content_key = msg.get("_content_key")
                if content_key and self.embedding_cache:
                    cached_embedding = self.embedding_cache.get(content_key)
                    if cached_embedding:
                        self.stats.cache_hits += 1
                        return item["idx"], cached_embedding

                self.stats.cache_misses += 1

                if self.has_images_in_content(msg_content):
                    msg_vector = await self.get_multimodal_embedding(
                        msg_content, progress.event_emitter
                    )
                    if not msg_vector:
                        msg_vector = await self.get_text_embedding(
                            msg_text, progress.event_emitter
                        )
                else:
                    msg_vector = await self.get_text_embedding(
                        msg_text, progress.event_emitter
                    )

                if content_key and msg_vector and self.embedding_cache:
                    self.embedding_cache.set(content_key, msg_vector)

                return item["idx"], msg_vector

        self.stats.concurrent_tasks = len(msg_items)
        embedding_tasks = [get_msg_embedding(item) for item in msg_items]
        embedding_results = await asyncio.gather(
            *embedding_tasks, return_exceptions=True
        )

        scored = []
        for item in msg_items:
            msg_vector = None
            for result in embedding_results:
                if isinstance(result, Exception):
                    continue
                result_idx, vector = result
                if result_idx == item["idx"]:
                    msg_vector = vector
                    break

            msg = item["msg"]
            msg_text = self.extract_text_from_content(msg.get("content", ""))

            sim = (
                self.cosine_similarity(query_vector, msg_vector)
                if (query_vector and msg_vector)
                else 0.0
            )

            recency = item["idx"] / max(1, len(msg_items) - 1)
            role = msg.get("role", "")
            role_weight = (
                1.0 if role == "user" else (0.8 if role == "assistant" else 0.6)
            )
            kw_bonus = 1.0 if self.is_high_priority_content(msg_text) else 0.0

            score = 0.6 * sim + 0.2 * recency + 0.1 * role_weight + 0.1 * kw_bonus

            scored.append(
                {
                    "msg": msg,
                    "score": score,
                    "tokens": item["tokens"],
                    "idx": item["idx"],
                    "sim": sim,
                    "recency": recency,
                    "role_weight": role_weight,
                    "kw_bonus": kw_bonus,
                }
            )

        return scored

    # ========== å‡çº§ç­–ç•¥ ==========

    def select_preserve_upgrades_with_protection(
        self, scored_msgs: List[dict], coverage_entries: List[dict], total_budget: int
    ) -> Tuple[set, int]:
        """é€‰æ‹©å‡çº§çš„æ¶ˆæ¯"""
        upgrade_pool = int(total_budget * self.valves.upgrade_min_pct)
        if upgrade_pool <= 0 or not scored_msgs:
            return set(), 0

        self.debug_log(
            1,
            f"å‡çº§æ± ä¿æŠ¤: é¢„ç•™{upgrade_pool:,}tokens({self.valves.upgrade_min_pct:.1%})ç»™å‡çº§",
            "â¬†ï¸",
        )

        summary_cost_map = defaultdict(int)
        for entry in coverage_entries:
            if entry["type"] == "micro":
                summary_cost_map[entry["msg_id"]] = entry.get(
                    "budget", entry.get("ideal_budget", 0)
                )

        candidates = []
        for item in scored_msgs:
            msg = item["msg"]
            msg_id = msg.get("_order_id", f"msg_{item['idx']}")
            original_tokens = item["tokens"]
            summary_cost = summary_cost_map.get(msg_id, 0)

            if summary_cost > 0:
                upgrade_cost = max(0, original_tokens - summary_cost)
            else:
                upgrade_cost = original_tokens

            if upgrade_cost <= 0:
                continue

            score = item["score"]
            if item["recency"] > 0.8:
                recency_boost = min(1.2, 1.0 + 0.2 * (2000 / max(upgrade_cost, 1)))
                score *= recency_boost

            density = score / upgrade_cost
            candidates.append(
                {
                    "density": density,
                    "score": score,
                    "upgrade_cost": upgrade_cost,
                    "item": item,
                    "msg_id": msg_id,
                }
            )

        candidates.sort(key=lambda x: (-x["density"], -x["score"]))

        preserve_set = set()
        consumed = 0
        self.debug_log(
            2, f"å‡çº§å€™é€‰: {len(candidates)}ä¸ªï¼Œå‡çº§æ± é¢„ç®—{upgrade_pool:,}tokens", "â¬†ï¸"
        )

        for cand in candidates:
            if consumed + cand["upgrade_cost"] > upgrade_pool:
                continue
            preserve_set.add(cand["msg_id"])
            consumed += cand["upgrade_cost"]
            self.debug_log(
                3,
                f"å‡çº§é€‰ä¸­: ID={cand['msg_id'][:8]}, å¯†åº¦={cand['density']:.4f}, æˆæœ¬={cand['upgrade_cost']}tokens",
                "â¬†ï¸",
            )

        self.debug_log(
            1,
            f"å‡çº§é€‰æ‹©å®Œæˆ: {len(preserve_set)}æ¡æ¶ˆæ¯å‡çº§, æ¶ˆè€—{consumed:,}/{upgrade_pool:,}tokens",
            "â¬†ï¸",
        )
        return preserve_set, consumed

    # ========== æ‘˜è¦ç”Ÿæˆ ==========

    async def generate_micro_summary_with_budget_impl(
        self, msg: dict, budget: int, event_emitter
    ):
        """ç”Ÿæˆå•æ¡æ¶ˆæ¯çš„å¾®æ‘˜è¦"""
        client = self.get_api_client()
        if not client:
            return None

        content = self.extract_text_from_content(msg.get("content", ""))
        role = msg.get("role", "")
        cleaned_content = self.input_cleaner.clean_text_for_regex(content)

        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ¶ˆæ¯ç”Ÿæˆç®€æ´æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. ä¿ç•™æ—¶é—´ã€ä¸»ä½“ã€åŠ¨ä½œã€æ•°æ®/ä»£ç å…³é”®è¡Œç­‰æ ¸å¿ƒè¦ç´ 
3. å¦‚æœæ˜¯æŠ€æœ¯å†…å®¹ï¼Œä¿ç•™æŠ€æœ¯æœ¯è¯­å’Œå…³é”®å‚æ•°
4. ä¿æŒå®¢è§‚ç®€æ´
æ¶ˆæ¯è§’è‰²: {role}
æ¶ˆæ¯å†…å®¹: {cleaned_content[:2000]}
æ‘˜è¦ï¼š"""

        has_multimodal = self.has_images_in_content(msg.get("content"))
        model_to_use = (
            self.valves.multimodal_model if has_multimodal else self.valves.text_model
        )
        self.stats.summary_requests += 1

        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,
            temperature=0.2,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary
        return None

    async def generate_adaptive_block_summary_impl(
        self, msgs: List[dict], idx_range: Tuple[int, int], budget: int, event_emitter
    ):
        """ç”Ÿæˆè‡ªé€‚åº”å—æ‘˜è¦"""
        client = self.get_api_client()
        if not client:
            return None

        combined_content = ""
        has_multimodal = False
        for i, msg in enumerate(msgs):
            role = msg.get("role", "")
            content = self.extract_text_from_content(msg.get("content", ""))
            combined_content += f"[æ¶ˆæ¯{idx_range[0] + i}:{role}] {content}\n\n"
            if self.has_images_in_content(msg.get("content")):
                has_multimodal = True

        cleaned_content = self.input_cleaner.clean_text_for_regex(combined_content)

        prompt = f"""è¯·ä¸ºä»¥ä¸‹è¿ç»­æ¶ˆæ¯å—(ç¬¬{idx_range[0]}åˆ°{idx_range[1]}æ¡)ç”Ÿæˆç»¼åˆæ‘˜è¦ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. è¦†ç›–æ‰€æœ‰è¦ç‚¹ï¼Œä¿æŒé€»è¾‘é¡ºåº
3. æŒ‡æ˜æ¶ˆæ¯ç¼–å·èŒƒå›´å’Œä¸»è¦è§’è‰²
4. ä¿ç•™å…³é”®æŠ€æœ¯ç»†èŠ‚ã€æ•°æ®ã€å‚æ•°ç­‰
æ¶ˆæ¯å—å†…å®¹ï¼š
{cleaned_content[:4000]}
å—æ‘˜è¦ï¼š"""

        model_to_use = (
            self.valves.multimodal_model if has_multimodal else self.valves.text_model
        )
        self.stats.summary_requests += 1

        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,
            temperature=0.2,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary
        return None

    async def generate_global_block_summary_impl(
        self, msgs: List[dict], idx_range: Tuple[int, int], budget: int, event_emitter
    ):
        """ç”Ÿæˆå…¨å±€å—æ‘˜è¦"""
        client = self.get_api_client()
        if not client:
            return None

        sampled_msgs = msgs[:: max(1, len(msgs) // 10)]
        combined_content = ""
        has_multimodal = False
        for i, msg in enumerate(sampled_msgs):
            role = msg.get("role", "")
            content = self.extract_text_from_content(msg.get("content", ""))
            combined_content += f"[æ¶ˆæ¯æ ·æœ¬{i}:{role}] {content[:200]}...\n\n"
            if self.has_images_in_content(msg.get("content")):
                has_multimodal = True

        cleaned_content = self.input_cleaner.clean_text_for_regex(combined_content)

        prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯å†å²ç”Ÿæˆå…¨å±€æ‘˜è¦ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åœ¨{budget}ä¸ªtokensä»¥å†…
2. æ¦‚æ‹¬ä¸»è¦è¯é¢˜å’Œè®¨è®ºè¦ç‚¹
3. ä¿ç•™é‡è¦çš„æŠ€æœ¯ç»†èŠ‚å’Œç»“è®º
4. æ€»å…±æ¶µç›–{len(msgs)}æ¡å†å²æ¶ˆæ¯
å¯¹è¯å†å²æ ·æœ¬ï¼š
{cleaned_content[:5000]}
å…¨å±€æ‘˜è¦ï¼š"""

        model_to_use = (
            self.valves.multimodal_model if has_multimodal else self.valves.text_model
        )
        self.stats.summary_requests += 1

        response = await client.chat.completions.create(
            model=model_to_use,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=budget,
            temperature=0.3,
            timeout=self.valves.request_timeout,
        )

        if response.choices and response.choices[0].message.content:
            summary = response.choices[0].message.content.strip()
            summary = self.input_cleaner.clean_text_for_regex(summary)
            return summary
        return None

    async def generate_coverage_summaries_with_budgets(
        self, coverage_entries: List[dict], progress: ProgressTracker
    ) -> Dict[str, str]:
        """å¹¶å‘ç”Ÿæˆè¦†ç›–æ‘˜è¦"""
        if not coverage_entries:
            return {}

        self.debug_log(1, f"å¼€å§‹å¹¶å‘ç”Ÿæˆè¦†ç›–æ‘˜è¦: {len(coverage_entries)}ä¸ªæ¡ç›®", "ğŸ“")

        summaries = {}
        semaphore = asyncio.Semaphore(self.valves.max_concurrent_requests)

        async def generate_single_summary(entry):
            async with semaphore:
                if entry["type"] == "micro":
                    msg = entry["msg"]
                    budget = entry.get(
                        "budget",
                        entry.get(
                            "ideal_budget", self.valves.coverage_high_summary_tokens
                        ),
                    )
                    msg_id = entry["msg_id"]
                    summary = await self.safe_api_call(
                        self.generate_micro_summary_with_budget_impl,
                        "å¾®æ‘˜è¦ç”Ÿæˆ",
                        msg,
                        budget,
                        progress.event_emitter,
                    )
                    if summary:
                        self.stats.coverage_micro_summaries += 1
                        return msg_id, summary
                    else:
                        content = self.extract_text_from_content(msg.get("content", ""))
                        fallback_summary = (
                            content[: budget * 3] + "..."
                            if len(content) > budget * 3
                            else content
                        )
                        self.stats.guard_b_fallbacks += 1
                        return msg_id, f"[ç®€åŒ–æ‘˜è¦] {fallback_summary}"

                elif entry["type"] == "adaptive_block":
                    msgs = entry["msgs"]
                    idx_range = entry["idx_range"]
                    budget = entry.get(
                        "budget",
                        entry.get(
                            "ideal_budget", self.valves.coverage_block_summary_tokens
                        ),
                    )
                    block_key = entry["block_key"]
                    summary = await self.safe_api_call(
                        self.generate_adaptive_block_summary_impl,
                        "è‡ªé€‚åº”å—æ‘˜è¦ç”Ÿæˆ",
                        msgs,
                        idx_range,
                        budget,
                        progress.event_emitter,
                    )
                    if summary:
                        self.stats.coverage_block_summaries += 1
                        self.stats.adaptive_blocks_created += 1
                        return block_key, summary
                    else:
                        combined = " ".join(
                            [
                                f"[{msg.get('role','')}]{self.extract_text_from_content(msg.get('content',''))[:100]}..."
                                for msg in msgs
                            ]
                        )
                        self.stats.guard_b_fallbacks += 1
                        return (
                            block_key,
                            f"[ç®€åŒ–å—æ‘˜è¦] ç¬¬{idx_range[0]}-{idx_range[1]}æ¡: {combined}",
                        )

                elif entry["type"] == "global_block":
                    msgs = entry["msgs"]
                    idx_range = entry["idx_range"]
                    budget = entry.get("budget", self.valves.min_block_summary_tokens)
                    block_key = entry["block_key"]
                    summary = await self.safe_api_call(
                        self.generate_global_block_summary_impl,
                        "å…¨å±€å—æ‘˜è¦ç”Ÿæˆ",
                        msgs,
                        idx_range,
                        budget,
                        progress.event_emitter,
                    )
                    if summary:
                        self.stats.coverage_block_summaries += 1
                        return block_key, summary
                    else:
                        self.stats.guard_b_fallbacks += 1
                        return (
                            block_key,
                            f"[å…¨å±€ç®€åŒ–æ‘˜è¦] åŒ…å«{len(msgs)}æ¡å†å²æ¶ˆæ¯çš„å¯¹è¯å†…å®¹",
                        )

                return None, None

        tasks = [generate_single_summary(entry) for entry in coverage_entries]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, Exception):
                self.stats.api_failures += 1
                continue
            key, summary = result
            if key and summary:
                summaries[key] = summary

        self.debug_log(1, f"å¹¶å‘æ‘˜è¦ç”Ÿæˆå®Œæˆ: {len(summaries)}ä¸ªæ‘˜è¦", "ğŸ“")
        return summaries

    # ========== ç»„è£…é˜¶æ®µåŒé‡æŠ¤æ  ==========

    async def assemble_coverage_output_with_guards(
        self,
        history_messages: List[dict],
        preserve_set: set,
        coverage_entries: List[dict],
        summaries: Dict[str, str],
        progress: ProgressTracker,
    ) -> List[dict]:
        """ç»„è£…æœ€ç»ˆè¾“å‡ºï¼ˆåŒé‡æŠ¤æ ç‰ˆæœ¬ï¼‰"""
        if not history_messages:
            return []

        self.debug_log(1, f"å¼€å§‹ç»„è£…æœ€ç»ˆè¾“å‡º: {len(history_messages)}æ¡å†å²æ¶ˆæ¯", "ğŸ”§")

        micro_entries = [e for e in coverage_entries if e["type"] == "micro"]
        adaptive_block_entries = [
            e for e in coverage_entries if e["type"] == "adaptive_block"
        ]
        global_block_entries = [
            e for e in coverage_entries if e["type"] == "global_block"
        ]

        if self.valves.debug_level >= 2:
            print(f"ğŸ›¡ï¸ æŠ¤æ Aç»Ÿè®¡:")
            print(f"    â”œâ”€ åŸæ–‡ä¿ç•™é›†åˆ: {len(preserve_set)}æ¡")
            print(f"    â”œâ”€ å¾®æ‘˜è¦æ¡ç›®: {len(micro_entries)}æ¡")
            print(f"    â”œâ”€ è‡ªé€‚åº”å—æ¡ç›®: {len(adaptive_block_entries)}æ¡")
            print(f"    â”œâ”€ å…¨å±€å—æ¡ç›®: {len(global_block_entries)}æ¡")
            print(f"    â”œâ”€ ç”Ÿæˆæ‘˜è¦æ€»æ•°: {len(summaries)}ä¸ª")
            print(f"    â””â”€ å†å²æ¶ˆæ¯æ€»æ•°: {len(history_messages)}æ¡")

        all_micro_msg_ids = {e["msg_id"] for e in micro_entries}
        all_msg_ids = {
            msg.get("_order_id", f"msg_{i}") for i, msg in enumerate(history_messages)
        }
        unmapped_msg_ids = all_msg_ids - all_micro_msg_ids

        if unmapped_msg_ids and self.valves.debug_level >= 2:
            unmapped_sample = list(unmapped_msg_ids)[:3]
            print(
                f"ğŸ›¡ï¸ æŠ¤æ Aè­¦å‘Š: {len(unmapped_msg_ids)}æ¡æ¶ˆæ¯æœªæ˜ å°„åˆ°å¾®æ‘˜è¦: {unmapped_sample}..."
            )
            self.stats.guard_a_warnings += 1

        msg_id_to_msg = {
            msg.get("_order_id", f"msg_{i}"): msg
            for i, msg in enumerate(history_messages)
        }

        block_summaries = {}
        block_ranges = {}
        entry_idx_ranges = {}

        for entry in adaptive_block_entries + global_block_entries:
            idx_range = entry["idx_range"]
            block_key = entry.get("block_key", f"block_{idx_range[0]}_{idx_range[1]}")
            entry_idx_ranges[block_key] = idx_range

            if block_key in summaries:
                block_summaries[block_key] = summaries[block_key]
                for idx in range(idx_range[0], idx_range[1] + 1):
                    if idx < len(history_messages):
                        block_ranges[idx] = block_key

        covered_by_micro_or_preserve = set()
        for i, msg in enumerate(history_messages):
            mid = msg.get("_order_id", f"msg_{i}")
            if mid in preserve_set or mid in summaries:
                covered_by_micro_or_preserve.add(i)

        final_messages = []
        processed_block_keys = set()
        covered_messages = 0

        for idx, msg in enumerate(history_messages):
            msg_id = msg.get("_order_id", f"msg_{idx}")
            message_covered = False

            if msg_id in preserve_set:
                final_messages.append(msg)
                self.stats.coverage_preserved_count += 1
                self.stats.coverage_preserved_tokens += self.count_message_tokens(msg)
                self.debug_log(3, f"ä½¿ç”¨åŸæ–‡: {msg_id[:8]}", "ğŸ“„")
                message_covered = True

            elif msg_id in summaries:
                summary_msg = {
                    "role": "assistant",
                    "content": summaries[msg_id],
                    "_is_summary": True,
                    "_original_msg_id": msg_id,
                    "_summary_type": "micro",
                }
                final_messages.append(summary_msg)
                self.stats.coverage_summary_count += 1
                self.stats.coverage_summary_tokens += self.count_message_tokens(
                    summary_msg
                )
                self.debug_log(3, f"ä½¿ç”¨å¾®æ‘˜è¦: {msg_id[:8]}", "ğŸ“„")
                message_covered = True

            elif idx in block_ranges:
                block_key = block_ranges[idx]
                if (
                    block_key not in processed_block_keys
                    and block_key in block_summaries
                ):
                    idx0, idx1 = entry_idx_ranges[block_key]
                    has_uncovered = any(
                        j not in covered_by_micro_or_preserve
                        for j in range(idx0, idx1 + 1)
                        if j < len(history_messages)
                    )
                    if has_uncovered:
                        block_summary_msg = {
                            "role": "assistant",
                            "content": block_summaries[block_key],
                            "_is_summary": True,
                            "_block_key": block_key,
                            "_summary_type": (
                                "adaptive_block"
                                if "global" not in block_key
                                else "global_block"
                            ),
                        }
                        final_messages.append(block_summary_msg)
                        processed_block_keys.add(block_key)
                        self.stats.coverage_summary_count += 1
                        self.stats.coverage_summary_tokens += self.count_message_tokens(
                            block_summary_msg
                        )
                        self.debug_log(3, f"ä½¿ç”¨å—æ‘˜è¦: {block_key}", "ğŸ“„")
                        for j in range(idx0, idx1 + 1):
                            if j < len(history_messages):
                                message_covered = True
                                break

                if idx in block_ranges:
                    message_covered = True

            else:
                self.debug_log(
                    1, f"æŠ¤æ Bè§¦å‘ï¼šæ¶ˆæ¯{msg_id[:8]}æ—¢ä¸åœ¨preserveä¹Ÿä¸åœ¨coverageä¸­", "ğŸ›¡ï¸"
                )
                content = self.extract_text_from_content(msg.get("content", ""))
                fallback_msg = {
                    "role": "assistant",
                    "content": f"[æŠ¤æ Bç®€åŒ–æ‘˜è¦] {content[:200]}...",
                    "_is_summary": True,
                    "_original_msg_id": msg_id,
                    "_summary_type": "guard_b_fallback",
                }
                final_messages.append(fallback_msg)
                self.stats.guard_b_fallbacks += 1
                self.stats.coverage_summary_count += 1
                self.stats.coverage_summary_tokens += self.count_message_tokens(
                    fallback_msg
                )
                message_covered = True

            if message_covered:
                covered_messages += 1

        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        self.stats.coverage_total_messages = len(history_messages)
        self.stats.coverage_rate = covered_messages / max(1, len(history_messages))

        final_tokens = self.count_messages_tokens(final_messages)

        if self.valves.debug_level >= 2:
            print(f"ğŸ›¡ï¸ æŠ¤æ Aæœ€ç»ˆéªŒè¯:")
            print(
                f"    â”œâ”€ æœ€ç»ˆæ¶ˆæ¯æ•°: åŸæ–‡{self.stats.coverage_preserved_count}æ¡ + æ‘˜è¦{self.stats.coverage_summary_count}æ¡ = {len(final_messages)}æ¡"
            )
            print(
                f"    â”œâ”€ è¦†ç›–ç‡éªŒè¯: {self.stats.coverage_rate:.1%} ({covered_messages}/{len(history_messages)})"
            )
            print(f"    â””â”€ æœ€ç»ˆtokenç»Ÿè®¡: {final_tokens:,}tokens")

        self.debug_log(
            1,
            f"åŒé‡æŠ¤æ ç»„è£…å®Œæˆ: {len(history_messages)} -> {len(final_messages)}æ¡æ¶ˆæ¯({final_tokens:,}tokens)",
            "âœ…",
        )
        return final_messages

    # ========== Top-upçª—å£å¡«å……å™¨ ==========

    def topup_fill_window(
        self,
        final_messages: List[dict],
        scored_msgs: List[dict],
        available_tokens: int,
        summaries: Dict[str, str],
        preserve_set: set,
    ) -> List[dict]:
        """Top-upå¡«å……å™¨"""
        initial_tokens = self.count_messages_tokens(final_messages)
        current_tokens = initial_tokens
        target_tokens = int(available_tokens * self.valves.target_window_usage)

        if current_tokens >= target_tokens:
            self.debug_log(
                1,
                f"çª—å£åˆ©ç”¨ç‡å·²è¾¾æ ‡: {current_tokens:,}/{target_tokens:,} tokens ({self.valves.target_window_usage:.1%})",
                "ğŸ”¥",
            )
            return final_messages

        self.debug_log(
            1,
            f"å¼€å§‹Top-upå¡«å……: {current_tokens:,} -> {target_tokens:,} tokens (ç›®æ ‡{self.valves.target_window_usage:.1%})",
            "ğŸ”¥",
        )
        self.stats.topup_applied += 1

        taken_micro = {
            m.get("_original_msg_id")
            for m in final_messages
            if m.get("_summary_type") == "micro"
        }
        id2msg = {
            item["msg"].get("_order_id", f"msg_{item['idx']}"): item
            for item in scored_msgs
        }

        micro_ids_sorted = sorted(
            [mid for mid in taken_micro if mid in id2msg],
            key=lambda mid: id2msg[mid]["score"] / max(1, id2msg[mid]["tokens"]),
            reverse=True,
        )

        upgraded_count = 0
        for mid in micro_ids_sorted:
            item = id2msg[mid]
            raw_msg = item["msg"]
            raw_tokens = self.count_message_tokens(raw_msg)

            micro_msg = None
            for i, msg in enumerate(final_messages):
                if msg.get("_original_msg_id") == mid:
                    micro_msg = msg
                    break

            if not micro_msg:
                continue

            micro_tokens = self.count_message_tokens(micro_msg)
            token_diff = raw_tokens - micro_tokens

            if current_tokens + token_diff > available_tokens:
                continue

            final_messages = [
                m for m in final_messages if m.get("_original_msg_id") != mid
            ]
            final_messages.append(raw_msg)
            current_tokens += token_diff
            upgraded_count += 1
            self.stats.topup_micro_upgraded += 1
            self.debug_log(
                3, f"å¾®æ‘˜è¦å‡çº§ä¸ºåŸæ–‡: {mid[:8]}, å¢åŠ {token_diff}tokens", "â¬†ï¸"
            )

            if current_tokens >= target_tokens:
                break

        if upgraded_count > 0:
            self.debug_log(1, f"å¾®æ‘˜è¦å‡çº§å®Œæˆ: {upgraded_count}æ¡å‡çº§", "â¬†ï¸")

        landed_ids = {
            m.get("_order_id") or m.get("_original_msg_id") for m in final_messages
        }
        candidates = [
            it for it in scored_msgs if it["msg"].get("_order_id") not in landed_ids
        ]
        candidates.sort(key=lambda it: it["score"] / max(1, it["tokens"]), reverse=True)

        added_count = 0
        for item in candidates:
            tokens = item["tokens"]
            if current_tokens + tokens > available_tokens:
                continue
            final_messages.append(item["msg"])
            current_tokens += tokens
            added_count += 1
            self.stats.topup_raw_added += 1
            self.debug_log(
                3,
                f"æ·»åŠ æœªè½åœ°åŸæ–‡: {item['msg'].get('_order_id', 'unknown')[:8]}, å¢åŠ {tokens}tokens",
                "ğŸ“",
            )
            if current_tokens >= target_tokens:
                break

        if added_count > 0:
            self.debug_log(1, f"æœªè½åœ°åŸæ–‡æ·»åŠ å®Œæˆ: {added_count}æ¡æ·»åŠ ", "ğŸ“")

        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        final_tokens = self.count_messages_tokens(final_messages)
        tokens_added = max(0, final_tokens - initial_tokens)
        self.stats.topup_tokens_added += tokens_added
        utilization = final_tokens / available_tokens if available_tokens > 0 else 0

        self.debug_log(
            1,
            f"Top-upå¡«å……å®Œæˆ: {final_tokens:,}tokens, åˆ©ç”¨ç‡{utilization:.1%}, æ–°å¢{tokens_added:,}tokens",
            "âœ…",
        )
        return final_messages

    # ========== ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¸»æµç¨‹ ==========

    async def process_coverage_first_context_maximization_v2(
        self,
        history_messages: List[dict],
        available_tokens: int,
        progress: ProgressTracker,
        query_message: dict,
        allow_topup: bool = False,
    ) -> List[dict]:
        """ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†ä¸»æµç¨‹"""
        if not history_messages or not self.valves.enable_coverage_first:
            return history_messages

        await progress.start_phase("ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†", len(history_messages))
        self.debug_log(
            1,
            f"ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¼€å§‹: {len(history_messages)}æ¡æ¶ˆæ¯, å¯ç”¨é¢„ç®—: {available_tokens:,}tokens",
            "ğŸ¯",
        )

        if self.valves.enable_smart_chunking:
            await progress.update_progress(0, 8, "æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†")
            processed_history = self.message_chunker.preprocess_messages_with_chunking(
                history_messages, self.message_order
            )
            self.stats.chunked_messages_count = len(
                [msg for msg in processed_history if msg.get("_is_chunk")]
            )
            self.stats.total_chunks_created = sum(
                1 for m in processed_history if m.get("_is_chunk")
            )
            self.debug_log(
                1,
                f"æ¶ˆæ¯åˆ†ç‰‡é¢„å¤„ç†: {len(history_messages)} -> {len(processed_history)}æ¡ ({self.stats.chunked_messages_count}æ¡è¢«åˆ†ç‰‡)",
                "ğŸ§©",
            )
        else:
            processed_history = history_messages

        await progress.update_progress(1, 8, "è®¡ç®—ç›¸å…³åº¦åˆ†æ•°")
        scored_msgs = await self.compute_relevance_scores(
            query_message, processed_history, progress
        )

        if not scored_msgs:
            self.debug_log(1, "ç›¸å…³åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯", "âš ï¸")
            return processed_history

        await progress.update_progress(2, 8, "è‡ªé€‚åº”Coverageè§„åˆ’")
        upgrade_pool = int(available_tokens * self.valves.upgrade_min_pct)
        coverage_budget = available_tokens - upgrade_pool
        coverage_entries, coverage_cost = (
            self.coverage_planner.plan_adaptive_coverage_summaries(
                scored_msgs, coverage_budget
            )
        )

        if coverage_cost < coverage_budget:
            actual_upgrade_pool = upgrade_pool + (coverage_budget - coverage_cost)
        else:
            actual_upgrade_pool = upgrade_pool

        if coverage_cost != coverage_budget:
            self.stats.budget_scaling_applied += 1
            self.stats.scaling_factor = (
                coverage_cost / coverage_budget if coverage_budget > 0 else 1.0
            )

        self.debug_log(
            1,
            f"è‡ªé€‚åº”Coverageè§„åˆ’: {len(coverage_entries)}ä¸ªæ¡ç›®, æˆæœ¬{coverage_cost:,}tokens (å‡çº§æ± {actual_upgrade_pool:,}tokens)",
            "ğŸ“„",
        )

        await progress.update_progress(3, 8, "å‡çº§ç­–ç•¥é€‰æ‹©")
        preserve_set, upgrade_consumed = self.select_preserve_upgrades_with_protection(
            scored_msgs, coverage_entries, actual_upgrade_pool
        )

        self.stats.coverage_upgrade_count = len(preserve_set)
        self.stats.coverage_upgrade_tokens_saved = upgrade_consumed

        await progress.update_progress(4, 8, "å¹¶å‘ç”Ÿæˆæ‘˜è¦å†…å®¹")
        summaries = await self.generate_coverage_summaries_with_budgets(
            coverage_entries, progress
        )

        await progress.update_progress(5, 8, "åŒé‡æŠ¤æ ç»„è£…")
        final_messages = await self.assemble_coverage_output_with_guards(
            processed_history, preserve_set, coverage_entries, summaries, progress
        )

        if allow_topup and self.valves.enable_window_topup:
            await progress.update_progress(6, 8, "Top-upçª—å£å¡«å……")
            final_messages = self.topup_fill_window(
                final_messages, scored_msgs, available_tokens, summaries, preserve_set
            )

        await progress.update_progress(7, 8, "æœ€ç»ˆç»Ÿè®¡è®¡ç®—")
        final_tokens = self.count_messages_tokens(final_messages)
        self.stats.coverage_budget_usage = (
            final_tokens / available_tokens if available_tokens > 0 else 0
        )

        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        await progress.update_progress(8, 8, "å¤„ç†å®Œæˆ")
        self.debug_log(
            1,
            f"ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å®Œæˆ: {len(processed_history)} -> {len(final_messages)}æ¡æ¶ˆæ¯",
            "âœ…",
        )
        self.debug_log(
            1,
            f"ç»Ÿè®¡: è¦†ç›–ç‡{self.stats.coverage_rate:.1%}, é¢„ç®—ä½¿ç”¨{self.stats.coverage_budget_usage:.1%}",
            "âœ…",
        )

        await progress.complete_phase(
            f"è¦†ç›–ç‡{self.stats.coverage_rate:.1%} é¢„ç®—ä½¿ç”¨{self.stats.coverage_budget_usage:.1%}"
        )
        return final_messages

    # ========== è§†è§‰å¤„ç† ==========

    def validate_base64_image_data(self, image_data: str) -> bool:
        """éªŒè¯base64å›¾ç‰‡æ•°æ®çš„æœ‰æ•ˆæ€§"""
        return self.input_cleaner.validate_and_clean_image_url(image_data)[0]

    async def describe_image_impl(self, image_data: str, event_emitter):
        """å®é™…çš„å›¾ç‰‡æè¿°å®ç°"""
        client = self.get_api_client()
        if not client:
            return None

        is_valid, cleaned_data = self.input_cleaner.validate_and_clean_image_url(
            image_data
        )
        if not is_valid:
            self.debug_log(1, "å›¾ç‰‡æ•°æ®éªŒè¯å¤±è´¥", "âš ï¸")
            self.stats.image_processing_errors += 1
            return "å›¾ç‰‡æ ¼å¼é”™è¯¯ï¼šä¸æ˜¯æœ‰æ•ˆçš„URLæˆ–data URI"

        try:
            response = await client.chat.completions.create(
                model=self.valves.multimodal_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": self.valves.vision_prompt_template,
                            },
                            {"type": "image_url", "image_url": {"url": cleaned_data}},
                        ],
                    }
                ],
                max_tokens=self.valves.vision_max_tokens,
                temperature=0.2,
                timeout=self.valves.request_timeout,
            )

            if response.choices and response.choices[0].message.content:
                description = response.choices[0].message.content.strip()
                description = self.input_cleaner.clean_text_for_regex(description)
                return description
            else:
                self.stats.image_processing_errors += 1
                return "å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼šAPIè¿”å›ç©ºå“åº”"
        except Exception as e:
            await self.learn_model_capability_from_errors(
                self.valves.multimodal_model, error_text=str(e)
            )
            self.debug_log(1, f"å›¾ç‰‡è¯†åˆ«å¼‚å¸¸: {str(e)[:100]}", "âŒ")
            self.stats.image_processing_errors += 1
            return f"å›¾ç‰‡è¯†åˆ«å¤±è´¥ï¼š{str(e)[:100]}"

    async def describe_image(self, image_data: str, event_emitter) -> str:
        """æè¿°å•å¼ å›¾ç‰‡"""
        if not image_data:
            return "å›¾ç‰‡æ•°æ®ä¸ºç©º"

        description = await self.safe_api_call(
            self.describe_image_impl,
            "å›¾ç‰‡è¯†åˆ«",
            image_data,
            event_emitter,
        )

        if description:
            if len(description) > 3000:
                description = description[:3000] + "..."
            return description
        else:
            self.stats.image_processing_errors += 1
            return "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼šæ— æ³•è·å–æè¿°"

    async def process_message_images(
        self, message: dict, progress: "ProgressTracker"
    ) -> dict:
        """å¤„ç†å•æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡"""
        content = message.get("content", "")
        if not isinstance(content, list):
            return message

        images = [item for item in content if item.get("type") == "image_url"]
        if not images:
            return message

        self.debug_log(2, f"å¤„ç†æ¶ˆæ¯ä¸­çš„å›¾ç‰‡: {len(images)}å¼ ", "ğŸ–¼ï¸")

        processed_content = []
        image_count = 0
        image_meta = []

        for item in content:
            if item.get("type") == "text":
                text = item.get("text", "")
                if text.strip():
                    processed_content.append(text)
            elif item.get("type") == "image_url":
                image_count += 1
                image_data = item.get("image_url", {}).get("url", "")

                is_valid, cleaned = self.input_cleaner.validate_and_clean_image_url(
                    image_data
                )
                if not is_valid:
                    self.stats.image_processing_errors += 1
                    processed_content.append(f"[å›¾ç‰‡{image_count}æ— æ³•è¯†åˆ«]")
                    continue

                if progress:
                    await progress.update_progress(
                        image_count,
                        len(images),
                        f"å¤„ç†å›¾ç‰‡ {image_count}/{len(images)}",
                    )

                description = await self.describe_image(
                    cleaned, progress.event_emitter if progress else None
                )

                image_name = f"img_{hashlib.md5(cleaned.encode()).hexdigest()[:8]}"
                image_line = f"[å›¾ç‰‡{image_count} {image_name}] {description}"
                processed_content.append(image_line)

                image_meta.append(
                    {
                        "index": image_count,
                        "name": image_name,
                        "source": "user",
                        "url": cleaned,
                    }
                )

        if image_count == 0:
            return message

        processed_message = copy.deepcopy(message)
        processed_message["content"] = (
            "\n".join(processed_content) if processed_content else ""
        )
        processed_message["_images_processed"] = image_count
        if image_meta:
            existing_meta = processed_message.get("_image_meta") or []
            processed_message["_image_meta"] = existing_meta + image_meta

        self.stats.multimodal_processed += image_count
        return processed_message

    def strip_images_from_message(self, message: dict) -> dict:
        """å°†å†å²æ¶ˆæ¯ä¸­çš„å›¾ç‰‡æ›¿æ¢ä¸ºå ä½ç¬¦"""
        content = message.get("content", "")
        if not isinstance(content, list):
            return message

        processed_parts = []
        image_count = 0
        meta_list = message.get("_image_meta") or []
        name_by_index = {
            m.get("index"): m.get("name")
            for m in meta_list
            if m.get("index") is not None
        }

        for item in content:
            if item.get("type") == "text":
                text = item.get("text", "")
                if text.strip():
                    processed_parts.append(text)
            elif item.get("type") == "image_url":
                image_count += 1
                tag_name = name_by_index.get(image_count)
                if tag_name:
                    processed_parts.append(f"[å†å²å›¾ç‰‡{image_count} {tag_name}]")
                else:
                    processed_parts.append(f"[å†å²å›¾ç‰‡{image_count}]")

        processed_message = copy.deepcopy(message)
        if processed_parts:
            processed_message["content"] = "\n".join(processed_parts)
        else:
            processed_message["content"] = "[å†å²å›¾ç‰‡å·²çœç•¥ï¼Œè¯·å‚è€ƒå¯¹è¯ä¸­çš„æ–‡å­—è¯´æ˜]"
        processed_message["_images_processed"] = image_count
        return processed_message

    # ========== å¤šæ¨¡æ€å¤„ç†ç­–ç•¥ ==========

    def calculate_multimodal_budget_sufficient(
        self, messages: List[dict], target_tokens: int
    ) -> bool:
        """è®¡ç®—å¤šæ¨¡æ€æ¨¡å‹çš„Tokené¢„ç®—æ˜¯å¦å……è¶³"""
        current_tokens = self.count_messages_tokens(messages)
        usage_ratio = current_tokens / target_tokens if target_tokens > 0 else 1.0
        threshold = self.valves.multimodal_direct_threshold
        is_sufficient = usage_ratio <= threshold
        self.debug_log(
            1,
            f"å¤šæ¨¡æ€é¢„ç®—æ£€æŸ¥: {current_tokens:,}/{target_tokens:,} = {usage_ratio:.2%} {'â‰¤' if is_sufficient else '>'} {threshold:.1%}",
            "ğŸ’°",
        )
        return is_sufficient

    async def determine_multimodal_processing_strategy(
        self, messages: List[dict], model_name: str, target_tokens: int
    ) -> Tuple[str, str]:
        """ç¡®å®šå¤šæ¨¡æ€å¤„ç†ç­–ç•¥"""
        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return "text_only", "æ— å›¾ç‰‡å†…å®¹ï¼ŒæŒ‰æ–‡æœ¬å¤„ç†"

        is_multimodal = self.is_multimodal_model(model_name)
        self.debug_log(1, f"æ¨¡å‹åˆ†æ: {model_name} | å¤šæ¨¡æ€æ”¯æŒ: {is_multimodal}", "ğŸ¤–")

        if is_multimodal:
            budget_sufficient = self.calculate_multimodal_budget_sufficient(
                messages, target_tokens
            )
            if budget_sufficient:
                return "direct_multimodal", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—å……è¶³ï¼Œç›´æ¥è¾“å…¥"
            else:
                return "multimodal_rag", "å¤šæ¨¡æ€æ¨¡å‹ï¼ŒTokené¢„ç®—ä¸è¶³ï¼Œä½¿ç”¨å¤šæ¨¡æ€å‘é‡RAG"
        else:
            return "vision_to_text", "çº¯æ–‡æœ¬æ¨¡å‹ï¼Œå…ˆè¯†åˆ«å›¾ç‰‡å†å¤„ç†"

    async def process_multimodal_content(
        self,
        messages: List[dict],
        model_name: str,
        target_tokens: int,
        progress: "ProgressTracker",
    ) -> List[dict]:
        """å¤šæ¨¡æ€å†…å®¹å¤„ç†"""
        if not self.valves.enable_multimodal:
            return messages

        has_images = self.has_images_in_messages(messages)
        if not has_images:
            return messages

        strategy, strategy_desc = await self.determine_multimodal_processing_strategy(
            messages, model_name, target_tokens
        )
        self.debug_log(1, f"å¤šæ¨¡æ€ç­–ç•¥: {strategy} - {strategy_desc}", "ğŸ¯")

        if strategy == "text_only":
            return messages
        elif strategy == "direct_multimodal":
            return messages
        elif strategy == "vision_to_text":
            await progress.start_phase("è§†è§‰è¯†åˆ«è½¬æ–‡æœ¬", 1)
            current_index = (
                self.message_order.find_current_user_message_index(messages)
                if self.message_order
                else -1
            )

            processed_messages: List[dict] = []
            for i, message in enumerate(messages):
                content = message.get("content")
                if not self.has_images_in_content(content):
                    processed_messages.append(message)
                    continue

                if i == current_index:
                    processed = await self.process_message_images(message, progress)
                    processed_messages.append(processed)
                else:
                    processed = self.strip_images_from_message(message)
                    processed_messages.append(processed)

            if self.message_order:
                processed_messages = self.message_order.sort_messages_preserve_user(
                    processed_messages, self.current_user_message
                )

            await progress.complete_phase("è§†è§‰è¯†åˆ«å®Œæˆ")
            return processed_messages
        else:
            return messages

    # ========== æ™ºèƒ½æˆªæ–­ ==========

    def smart_truncate_messages(
        self, messages: List[dict], target_tokens: int, preserve_priority: bool = True
    ) -> List[dict]:
        """æ™ºèƒ½æˆªæ–­ç®—æ³•"""
        if not messages:
            return messages

        current_tokens = self.count_messages_tokens(messages)
        if current_tokens <= target_tokens:
            return messages

        self.debug_log(
            1, f"å¼€å§‹æ™ºèƒ½æˆªæ–­: {current_tokens:,} -> {target_tokens:,}tokens", "âœ‚ï¸"
        )
        self.stats.smart_truncation_applied += 1

        if preserve_priority:
            message_priorities = []
            for i, msg in enumerate(messages):
                priority_score = self._calculate_message_priority(msg, i, len(messages))
                message_priorities.append((i, msg, priority_score))
            message_priorities.sort(key=lambda x: x[2], reverse=True)
        else:
            message_priorities = [(i, msg, 1.0) for i, msg in enumerate(messages)]

        selected_messages = []
        used_tokens = 0
        skipped_messages = []

        for original_idx, msg, priority in message_priorities:
            msg_tokens = self.count_message_tokens(msg)
            if used_tokens + msg_tokens <= target_tokens:
                selected_messages.append((original_idx, msg, priority))
                used_tokens += msg_tokens
            else:
                skipped_messages.append((original_idx, msg, priority, msg_tokens))
                self.stats.truncation_skip_count += 1

        remaining_budget = target_tokens - used_tokens
        if remaining_budget > 100 and skipped_messages:
            skipped_messages.sort(key=lambda x: x[3])
            recovered_count = 0
            for original_idx, msg, priority, msg_tokens in skipped_messages:
                if msg_tokens <= remaining_budget:
                    selected_messages.append((original_idx, msg, priority))
                    used_tokens += msg_tokens
                    remaining_budget -= msg_tokens
                    recovered_count += 1
                    if remaining_budget < 100:
                        break
            self.stats.truncation_recovered_messages += recovered_count

        selected_messages.sort(key=lambda x: x[0])
        final_messages = [msg for _, msg, _ in selected_messages]

        if self.message_order:
            final_messages = self.message_order.sort_messages_preserve_user(
                final_messages, self.current_user_message
            )

        final_tokens = self.count_messages_tokens(final_messages)
        retention_ratio = len(final_messages) / len(messages) if messages else 0
        self.debug_log(
            1,
            f"æ™ºèƒ½æˆªæ–­å®Œæˆ: {len(messages)} -> {len(final_messages)}æ¡æ¶ˆæ¯ ä¿ç•™ç‡{retention_ratio:.1%}",
            "âœ…",
        )
        return final_messages

    def _calculate_message_priority(
        self, msg: dict, index: int, total_count: int
    ) -> float:
        """è®¡ç®—æ¶ˆæ¯ä¼˜å…ˆçº§åˆ†æ•°"""
        priority = 1.0

        role = msg.get("role", "")
        if role == "user":
            priority += 2.0
        elif role == "assistant":
            priority += 1.5
        elif role == "system":
            priority += 3.0

        position_score = index / total_count if total_count > 0 else 0
        priority += position_score * 2.0

        content_text = self.extract_text_from_content(msg.get("content", ""))
        if self.is_high_priority_content(content_text):
            priority += 1.5

        content_length = len(content_text)
        if 100 < content_length < 2000:
            priority += 0.5
        elif content_length > 5000:
            priority -= 1.0
        elif content_length > 10000:
            priority -= 2.0

        if self.has_images_in_content(msg.get("content")):
            priority += 1.0

        if msg.get("_is_summary"):
            priority += 0.8

        if msg.get("_is_chunk"):
            priority += 0.3

        return priority

    # ========== ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤ ==========

    def ensure_current_user_message_preserved(
        self, final_messages: List[dict]
    ) -> List[dict]:
        """ç¡®ä¿å½“å‰ç”¨æˆ·æ¶ˆæ¯è¢«æ­£ç¡®ä¿ç•™åœ¨æœ€åä½ç½®"""
        if not self.current_user_message:
            return final_messages

        if final_messages and final_messages[-1].get("role") == "user":
            current_id = self.current_user_message.get("_order_id")
            last_id = final_messages[-1].get("_order_id")
            if current_id == last_id:
                return final_messages

        self.debug_log(1, "æ£€æµ‹åˆ°å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®é”™è¯¯ï¼Œå¼€å§‹ä¿®å¤", "ğŸ›¡ï¸")
        current_id = self.current_user_message.get("_order_id")
        filtered_messages = []
        for msg in final_messages:
            if msg.get("_order_id") != current_id:
                filtered_messages.append(msg)
        filtered_messages.append(self.current_user_message)

        self.stats.user_message_recovery_count += 1
        self.debug_log(1, "å½“å‰ç”¨æˆ·æ¶ˆæ¯ä½ç½®ä¿®å¤å®Œæˆ", "ğŸ›¡ï¸")
        return filtered_messages

    # ========== ä¸»è¦å¤„ç†é€»è¾‘ ==========

    async def maximize_content_comprehensive_processing_v2(
        self, messages: List[dict], target_tokens: int, progress: ProgressTracker
    ) -> List[dict]:
        """å†…å®¹æœ€å¤§åŒ–ç»¼åˆå¤„ç†"""
        start_time = time.time()

        current_model_name = getattr(self, "_current_model_name", "unknown")
        if hasattr(self, "current_model_info") and self.current_model_info:
            model_limit = self.current_model_info.get(
                "limit", self.valves.default_token_limit
            )
            safe_limit = int(model_limit * self.valves.token_safety_ratio)
        else:
            safe_limit = self.get_model_token_limit(current_model_name)

        self.stats.original_tokens = self.count_messages_tokens(messages)
        self.stats.original_messages = len(messages)
        self.stats.token_limit = safe_limit
        self.stats.target_tokens = target_tokens
        current_tokens = self.stats.original_tokens

        self.debug_log(
            1,
            f"ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†å¼€å§‹: {current_tokens:,} tokens, {len(messages)} æ¡æ¶ˆæ¯",
            "ğŸ¯",
        )

        await progress.start_phase("ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†", 10)

        await progress.update_progress(1, 10, "åˆ†ç¦»å½“å‰ç”¨æˆ·æ¶ˆæ¯å’Œå†å²æ¶ˆæ¯")
        current_user_message, history_messages = (
            self.separate_current_and_history_messages(messages)
        )
        self.current_user_message = current_user_message

        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        if current_user_message:
            self.stats.current_user_tokens = self.count_message_tokens(
                current_user_message
            )

        need_context_max = False
        if current_user_message and self.valves.enable_context_maximization:
            query_text = self.extract_text_from_content(
                current_user_message.get("content", "")
            )
            need_context_max = await self.detect_context_max_need(
                query_text, progress.event_emitter
            )
            if need_context_max:
                self.debug_log(1, f"æ£€æµ‹åˆ°éœ€è¦ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ï¼Œå¯ç”¨ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ç­–ç•¥", "ğŸ“š")

        protected_messages = system_messages[:]
        protected_tokens = self.count_messages_tokens(protected_messages)
        available_for_processing = (
            target_tokens - protected_tokens - self.stats.current_user_tokens
        )

        self.debug_log(
            1, f"å†å²æ¶ˆæ¯å¯ç”¨å¤„ç†ç©ºé—´: {available_for_processing:,}tokens", "ğŸ’°"
        )

        if not history_messages:
            final_messages = system_messages[:]
            if current_user_message:
                final_messages.append(current_user_message)
            await progress.complete_phase("æ— å†å²æ¶ˆæ¯éœ€è¦å¤„ç†")
            return final_messages

        if (
            need_context_max
            and self.valves.enable_context_maximization
            and self.valves.enable_coverage_first
        ):
            await progress.update_progress(2, 10, "ä¸Šä¸‹æ–‡æœ€å¤§åŒ–ä¸“ç”¨å¤„ç†")
            processed_history = (
                await self.process_coverage_first_context_maximization_v2(
                    history_messages,
                    available_for_processing,
                    progress,
                    current_user_message,
                    allow_topup=True and self.valves.enable_window_topup,
                )
            )
        else:
            await progress.update_progress(2, 10, "æ ‡å‡†æˆªæ–­å¤„ç†")
            if available_for_processing > 0:
                processed_history = self.smart_truncate_messages(
                    history_messages, available_for_processing, True
                )
            else:
                processed_history = []

        await progress.update_progress(6, 10, "ä¸æˆªæ–­ä¿éšœæ£€æŸ¥")
        final_history = processed_history
        final_tokens = self.count_messages_tokens(final_history)

        if (
            final_tokens > available_for_processing
            and self.valves.disable_insurance_truncation
        ):
            self.debug_log(1, f"é¢„ç®—è¶…é™ä½†ç¦ç”¨æˆªæ–­ï¼Œä¿è¯ä¸æˆªæ–­", "ğŸ›¡ï¸")
            self.stats.insurance_truncation_avoided += 1
        elif final_tokens > available_for_processing:
            self.debug_log(1, f"è¶…å‡ºé¢„ç®—ï¼Œå¯ç”¨ä¿é™©æˆªæ–­", "âœ‚ï¸")
            final_history = self.smart_truncate_messages(
                final_history, available_for_processing, True
            )
            final_tokens = self.count_messages_tokens(final_history)
            self.stats.zero_loss_guarantee = False

        await progress.update_progress(8, 10, "ç»„åˆæœ€ç»ˆç»“æœ")
        current_result = system_messages + final_history

        if self.message_order:
            current_result = self.message_order.sort_messages_preserve_user(
                current_result, self.current_user_message
            )

        final_messages = []
        for msg in current_result:
            final_messages.append(msg)
        if current_user_message:
            final_messages.append(current_user_message)

        await progress.update_progress(9, 10, "ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤éªŒè¯")
        final_messages = self.ensure_current_user_message_preserved(final_messages)

        await progress.update_progress(10, 10, "æ›´æ–°ç»Ÿè®¡")
        self.stats.final_tokens = self.count_messages_tokens(final_messages)
        self.stats.final_messages = len(final_messages)
        self.stats.processing_time = time.time() - start_time
        self.stats.iterations = 1

        if self.stats.original_tokens > 0:
            self.stats.content_loss_ratio = max(
                0,
                (self.stats.original_tokens - self.stats.final_tokens)
                / self.stats.original_tokens,
            )

        if target_tokens > 0:
            self.stats.window_utilization = self.stats.final_tokens / target_tokens

        if current_user_message:
            self.stats.current_user_preserved = any(
                msg.get("_order_id") == current_user_message.get("_order_id")
                for msg in final_messages
            )

        retention_ratio = self.stats.calculate_retention_ratio()
        window_usage = self.stats.calculate_window_usage_ratio()

        self.debug_log(
            1,
            f"ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†å®Œæˆ: ä¿ç•™{retention_ratio:.1%} çª—å£ä½¿ç”¨{window_usage:.1%} ä¸æˆªæ–­{'ä¿éšœæˆåŠŸ' if self.stats.zero_loss_guarantee else 'éƒ¨åˆ†å¤±æ•ˆ'}",
            "ğŸ¯",
        )

        await progress.complete_phase(
            f"è¦†ç›–ç‡{self.stats.coverage_rate:.1%} é¢„ç®—ä½¿ç”¨{window_usage:.1%} "
            f"ä¸æˆªæ–­{'æˆåŠŸ' if self.stats.zero_loss_guarantee else 'å¤±æ•ˆ'} "
            f"{'[ä¸Šä¸‹æ–‡æœ€å¤§åŒ–]' if need_context_max else '[å…·ä½“æŸ¥è¯¢]'}"
        )
        return final_messages

    def print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.valves.enable_detailed_stats:
            return
        print("\n" + "=" * 60)
        print(self.stats.get_summary())
        print("=" * 60)

    # ========== å…¥å£å’Œå‡ºå£å‡½æ•° ==========

    async def inlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
        __user__: Optional[dict] = None,
        **kwargs,
    ) -> dict:
        """å…¥å£å‡½æ•°"""
        if self.valves.debug_level >= 1:
            print("ğŸš€ é«˜çº§ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯åŠ¨ï¼ˆè‡ªåŠ¨è®°å¿†åå°è¿è¡Œï¼‰")

        # å…¼å®¹ä¸åŒç‰ˆæœ¬ä¼ å‚ï¼šuser / __user__
        user = (
            user
            if isinstance(user, dict)
            else (__user__ if isinstance(__user__, dict) else None)
        )
        if user is None and isinstance(kwargs.get("user"), dict):
            user = kwargs.get("user")
        if user is None and isinstance(kwargs.get("__user__"), dict):
            user = kwargs.get("__user__")
        try:
            if isinstance(user, dict) and user.get("id") is not None:
                self.current_user_obj = Users.get_user_by_id(user["id"])
        except Exception:
            pass

        # 1. è®°å¿†ä¸Šä¸‹æ–‡å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.valves.enable_auto_memory and self.valves.override_memory_context:
            if "messages" in body:
                try:
                    body["messages"] = self.process_memory_context_in_messages(
                        body["messages"]
                    )
                except Exception as e:
                    self.memory_log(f"è®°å¿†ä¸Šä¸‹æ–‡å¤„ç†å¤±è´¥: {e}", "error")

        # 2. åŸæœ‰çš„ä¸Šä¸‹æ–‡å¤„ç†é€»è¾‘
        if not self.valves.enable_processing:
            return body

        messages = body.get("messages", [])
        if not messages:
            return body

        model_name = body.get("model", "æœªçŸ¥")
        if self.is_model_excluded(model_name):
            return body

        self.reset_processing_state()
        self._current_model_name = model_name
        self.current_model_info = self.analyze_model(model_name)

        original_tokens = self.count_messages_tokens(messages)
        model_token_limit = self.get_model_token_limit(model_name)
        current_user_tokens = (
            self.count_message_tokens(self.find_current_user_message(messages))
            if self.find_current_user_message(messages)
            else 0
        )
        target_tokens = self.calculate_target_tokens(model_name, current_user_tokens)

        needs_proc, token_overflow, mm_incompat = self._needs_processing(
            messages, model_name, target_tokens
        )

        show_progress = needs_proc or not self.valves.suppress_frontend_when_idle
        progress = ProgressTracker(__event_emitter__ if show_progress else None)

        self.message_order = MessageOrder(messages)
        messages = self.message_order.original_messages

        current_user_message, history_messages = (
            self.separate_current_and_history_messages(messages)
        )
        self.current_user_message = current_user_message

        self.stats.token_limit = model_token_limit
        self.stats.target_tokens = target_tokens
        self.stats.current_user_tokens = current_user_tokens

        if self.valves.debug_level >= 1:
            print(
                f"æ¨¡å‹: {self.current_model_info['family']} | tokens: {original_tokens:,}/{model_token_limit:,} | å†å²: {len(history_messages)}æ¡"
            )

        if current_user_message:
            content_preview = self.message_order.get_message_preview(
                current_user_message
            )
            processing_id = hashlib.md5(
                f"{current_user_message.get('_order_id', '')}{content_preview}{time.time()}".encode()
            ).hexdigest()[:8]
            self.current_processing_id = processing_id

            need_context_max = False
            if False and self.valves.enable_ai_context_max_detection and needs_proc:
                query_text = self.extract_text_from_content(
                    current_user_message.get("content", "")
                )
                try:
                    need_context_max = await self.detect_context_max_need(
                        query_text, __event_emitter__
                    )
                    if self.valves.debug_level >= 1:
                        print(
                            f"ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹: {'éœ€è¦' if need_context_max else 'ä¸éœ€è¦'}"
                        )
                except Exception as e:
                    if self.valves.debug_level >= 1:
                        print(f"AIæ£€æµ‹å¤±è´¥: {e}")
                    need_context_max = self.is_context_max_need_simple(query_text)

        should_maximize = needs_proc

        try:
            if self.valves.enable_detailed_progress:
                await progress.start_phase("å¤šæ¨¡æ€å¤„ç†", 1)

            processed_messages = await self.process_multimodal_content(
                messages, model_name, target_tokens, progress
            )
            processed_tokens = self.count_messages_tokens(processed_messages)

            if not self.is_multimodal_model(model_name):
                _post_tmp_user = self.find_current_user_message(processed_messages)
                if _post_tmp_user is not None:
                    c = _post_tmp_user.get("content")
                    if not isinstance(c, str):
                        parts = []
                        if isinstance(c, list):
                            for it in c:
                                if isinstance(it, str):
                                    parts.append(it)
                                elif isinstance(it, dict):
                                    t = it.get("type")
                                    if t == "text" and isinstance(it.get("text"), str):
                                        parts.append(it["text"])
                                    elif t == "image_url":
                                        img = it.get("image_url")
                                        url = (
                                            img.get("url", "")
                                            if isinstance(img, dict)
                                            else (img if isinstance(img, str) else "")
                                        )
                                        parts.append(
                                            f"[å›¾ç‰‡] {url}" if url else "[å›¾ç‰‡]"
                                        )
                                    elif isinstance(it.get("content"), str):
                                        parts.append(it["content"])
                        elif isinstance(c, dict):
                            if c.get("type") == "text" and isinstance(
                                c.get("text"), str
                            ):
                                parts.append(c["text"])
                            elif c.get("type") == "image_url":
                                img = c.get("image_url")
                                url = (
                                    img.get("url", "")
                                    if isinstance(img, dict)
                                    else (img if isinstance(img, str) else "")
                                )
                                parts.append(f"[å›¾ç‰‡] {url}" if url else "[å›¾ç‰‡]")
                            elif isinstance(c.get("content"), str):
                                parts.append(c["content"])
                        _post_tmp_user["content"] = "\n".join(
                            p for p in parts if isinstance(p, str)
                        ).strip()

            processed_tokens = self.count_messages_tokens(processed_messages)

            _post_user = self.find_current_user_message(processed_messages)
            _post_user_tokens = (
                self.count_message_tokens(_post_user) if _post_user else 0
            )
            target_tokens = self.calculate_target_tokens(model_name, _post_user_tokens)
            post_needs_proc, post_token_overflow, post_mm_incompat = (
                self._needs_processing(processed_messages, model_name, target_tokens)
            )

            if not post_needs_proc:
                self.stats.original_tokens = self.count_messages_tokens(messages)
                self.stats.original_messages = len(messages)
                self.stats.final_tokens = processed_tokens
                self.stats.final_messages = len(processed_messages)
                body["messages"] = copy.deepcopy(processed_messages)
                body["messages"] = self.strip_internal_fields(body["messages"])
                if self.valves.debug_level >= 1:
                    print("æ— éœ€å¤„ç†ï¼šå¤šæ¨¡æ€è½¬å†™åæœªè¶…é™ï¼Œç›´æ¥è¿”å›åŸæ–‡ï¼ˆæˆ–è½¬å†™åï¼‰")
                return body

            should_maximize = post_needs_proc
            if (
                self.valves.enable_ai_context_max_detection
                and should_maximize
                and _post_user
            ):
                query_text = self.extract_text_from_content(
                    _post_user.get("content", "")
                )
                try:
                    need_context_max = await self.detect_context_max_need(
                        query_text, __event_emitter__
                    )
                    if self.valves.debug_level >= 1:
                        print(
                            f"ä¸Šä¸‹æ–‡æœ€å¤§åŒ–æ£€æµ‹(åç§»): {'éœ€è¦' if need_context_max else 'ä¸éœ€è¦'}"
                        )
                    if not need_context_max:
                        should_maximize = False
                except Exception as e:
                    if self.valves.debug_level >= 1:
                        print(f"AIæ£€æµ‹(åç§»)å¤±è´¥: {e}")

            if should_maximize:
                final_messages = (
                    await self.maximize_content_comprehensive_processing_v2(
                        processed_messages, target_tokens, progress
                    )
                )
                self.print_detailed_stats()
                body["messages"] = copy.deepcopy(final_messages)

                final_tokens = self.count_messages_tokens(final_messages)
                window_utilization = (
                    final_tokens / target_tokens if target_tokens > 0 else 0
                )

                if self.valves.debug_level >= 1:
                    print(
                        f"å¤„ç†å®Œæˆ: {len(final_messages)}æ¡æ¶ˆæ¯, {final_tokens:,}tokens, åˆ©ç”¨ç‡{window_utilization:.1%}, ä¸æˆªæ–­{'âœ…' if self.stats.zero_loss_guarantee else 'âš ï¸'}"
                    )

                if current_user_message and final_messages:
                    last_msg = final_messages[-1]
                    if last_msg.get("role") == "user" and self.valves.debug_level >= 1:
                        print(f"å½“å‰ç”¨æˆ·æ¶ˆæ¯ä¿æŠ¤: âœ…")
            else:
                self.stats.original_tokens = self.count_messages_tokens(messages)
                self.stats.original_messages = len(messages)
                self.stats.final_tokens = processed_tokens
                self.stats.final_messages = len(processed_messages)
                if self.valves.enable_detailed_progress:
                    await progress.complete_phase("æ— éœ€æœ€å¤§åŒ–å¤„ç†")
                body["messages"] = copy.deepcopy(processed_messages)
                if self.valves.debug_level >= 1:
                    print(f"ç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯")

        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            self.stats.api_failures += 1
            import traceback

            if self.valves.debug_level >= 2:
                traceback.print_exc()
            if self.valves.enable_detailed_progress:
                await progress.update_status(f"å¤„ç†å¤±è´¥: {str(e)[:50]}", True)

        if self.valves.debug_level >= 1:
            print("ğŸ ä¸Šä¸‹æ–‡æœ€å¤§åŒ–å¤„ç†å®Œæˆ")

        if isinstance(body.get("messages"), list):
            body["messages"] = self.strip_internal_fields(body["messages"])

        # ========== Auto Memory å…œåº•è§¦å‘ï¼ˆå…¼å®¹æŸäº›ç‰ˆæœ¬ outlet hook ä¸è§¦å‘ï¼‰ ==========
        try:
            if getattr(self.valves, "enable_auto_memory", False):
                msgs = body.get("messages") or []
                if isinstance(msgs, list) and len(msgs) >= 1:
                    # å°½é‡é¿å…å¤„ç†â€œå½“å‰æœªå®Œæˆçš„ä¸€è½®â€ï¼ˆé€šå¸¸ inlet æœ€åä¸€ä¸ªæ˜¯æœ¬è½® user æ¶ˆæ¯ï¼‰
                    cand = msgs
                    try:
                        if (
                            isinstance(msgs[-1], dict)
                            and msgs[-1].get("role") == "user"
                            and len(msgs) >= 2
                        ):
                            cand = msgs[:-1]
                            if len(cand) == 0:
                                cand = msgs
                    except Exception:
                        cand = msgs

                    # å»é‡ï¼šé¿å… inlet/outlet åŒè§¦å‘é‡å¤å†™å…¥
                    import json as _am_json
                    import hashlib as _am_hashlib
                    import time as _am_time

                    try:
                        sig_src = _am_json.dumps(
                            cand[-4:], ensure_ascii=False, sort_keys=True
                        )
                    except Exception:
                        sig_src = str(cand[-4:])
                    sig = _am_hashlib.sha1(
                        sig_src.encode("utf-8", errors="ignore")
                    ).hexdigest()
                    last_sig = getattr(self, "_am_last_sig", None)
                    last_ts = getattr(self, "_am_last_ts", 0.0)
                    now_ts = _am_time.time()
                    if sig != last_sig or (now_ts - float(last_ts)) > 30.0:
                        setattr(self, "_am_last_sig", sig)
                        setattr(self, "_am_last_ts", now_ts)

                        # å…¼å®¹ä¸åŒç‰ˆæœ¬ä¼ å‚ï¼šuser / __user__ / body["user"]
                        user_dict = user if isinstance(user, dict) else None
                        if user_dict is None and isinstance(body.get("__user__"), dict):
                            user_dict = body.get("__user__")
                        if user_dict is None and isinstance(body.get("user"), dict):
                            user_dict = body.get("user")

                        user_obj = None
                        try:
                            if (
                                isinstance(user_dict, dict)
                                and user_dict.get("id") is not None
                            ):
                                user_obj = Users.get_user_by_id(user_dict["id"])
                        except Exception:
                            user_obj = None

                        if user_obj is not None:
                            _run_detached(
                                self.auto_memory_process(
                                    cand, user_obj, __event_emitter__
                                ),
                                name="auto_memory_process_inlet",
                                logger=self.logger,
                            )
                            self.memory_log(
                                "inletå…œåº•: å·²å¯åŠ¨å¼‚æ­¥è®°å¿†å¤„ç†ï¼ˆå¦‚ç‰ˆæœ¬ä¸è§¦å‘outletä¹Ÿèƒ½å·¥ä½œï¼‰",
                                "info",
                            )
        except Exception as e:
            try:
                self.memory_log("inletå…œåº•è§¦å‘å¤±è´¥: %s" % e, "error")
            except Exception:
                pass

        return body

    async def outlet(
        self,
        body: dict,
        user: Optional[dict] = None,
        __event_emitter__: Optional[Callable] = None,
        __user__: Optional[dict] = None,
        **kwargs,
    ) -> dict:
        """å‡ºå£å‡½æ•° - æ·»åŠ å¼‚æ­¥è®°å¿†å¤„ç†"""
        # å…¼å®¹ä¸åŒç‰ˆæœ¬ä¼ å‚ï¼šuser / __user__ï¼Œä»¥åŠéƒ¨åˆ†ç‰ˆæœ¬ outlet ä¸ä¼  user çš„æƒ…å†µ
        user = (
            user
            if isinstance(user, dict)
            else (__user__ if isinstance(__user__, dict) else None)
        )
        if user is None and isinstance(kwargs.get("user"), dict):
            user = kwargs.get("user")
        if user is None and isinstance(kwargs.get("__user__"), dict):
            user = kwargs.get("__user__")

        user_obj = None
        try:
            if isinstance(user, dict) and user.get("id") is not None:
                user_obj = Users.get_user_by_id(user["id"])
        except Exception:
            user_obj = None

        if user_obj is None:
            user_obj = getattr(self, "current_user_obj", None)

        if user_obj is None:
            return body

        try:
            # user_obj å·²å‡†å¤‡å¥½

            if user_obj is None:
                self.memory_log("ç”¨æˆ·å¯¹è±¡è·å–å¤±è´¥", "error")
                return body

            self.current_user_obj = user_obj
        except Exception as e:
            self.memory_log(f"ç”¨æˆ·ä¿¡æ¯è·å–å¼‚å¸¸: {e}", "error")
            return body

        if self.valves.enable_auto_memory:
            messages = body.get("messages", [])
            if messages and len(messages) >= 2:
                _run_detached(
                    self.auto_memory_process(
                        messages,
                        user_obj,
                        __event_emitter__,
                    ),
                    name="auto_memory_process",
                    logger=self.logger,
                )
                self.memory_log("å·²å¯åŠ¨å¼‚æ­¥è®°å¿†å¤„ç†", "info")

        return body
